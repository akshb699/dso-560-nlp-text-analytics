{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3\n",
    "\n",
    "Submit via Slack. Due on **Tuesday, April 12th, 2022, 6:29pm PST**. You may work with one other person.\n",
    "## TF-IDF (5pts)\n",
    "\n",
    "You are an analyst working for Amazon's product team, and charged with identifying areas for improvement for the toy reviews.\n",
    "\n",
    "Using the **amazon-fine-foods.csv** dataset, clean and parse the text reviews. Explain the decisions you make:\n",
    "- why remove/keep stopwords?\n",
    "- which stopwords to remove?\n",
    "- stemming versus lemmatization?\n",
    "- regex cleaning and substitution?\n",
    "- adding in custom stopwords?\n",
    "- what `n` for your `n-grams`?\n",
    "\n",
    "Finally, generate a TF-IDF report that explains for a business (non-technical) stakeholder:\n",
    "* the features your analysis showed that customers cited as reasons for a poor review\n",
    "* the features your analysis showed that customers cited as reasons for a good review\n",
    "* the most common issues identified from your analysis that generated customer dissatisfaction.\n",
    "\n",
    "Explain to what degree the TF-IDF findings make sense - what are its limitations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize text into words. Convert texts to lower case.\n",
    "    Remove hashtags, punctuations, stopwords, website links, extra spaces, non-alphanumeric characters and \n",
    "    single character. stemtize texts.\n",
    "    \"\"\"\n",
    "\n",
    "    # erase html language characters\n",
    "    html = re.compile(r'<.*?>')\n",
    "    text = html.sub(r'',text)\n",
    "\n",
    "    # product replacement\n",
    "    text = re.sub(r'\\b(cookiee?s?)\\b', 'FOOD_PRODUCT_1', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(greenies?)\\b', 'FOOD_PRODUCT_2', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(coconut\\s?oils?|oils?)\\b', 'FOOD_PRODUCT_3', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # numerbers plus units\n",
    "    text = re.sub(r'\\b([1-9]+[\\w]*)\\b', '_NUMERIC_', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # taste words\n",
    "    text = re.sub(r'\\b(tasty|delicious|yum*y|enjoy)\\b', 'GOOD_TASTE', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(nasty|disgusting|rotten|stale)\\b', 'BAD_TASTE', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # words with 3 or more of same letters\n",
    "    text = re.sub(r'\\b[a-z0-9]*(.)\\1\\1+[a-z0-9]*\\b', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    tokens = [token for token in nltk.word_tokenize(text)]\n",
    "    \n",
    "    # Combine stopwords and punctuation\n",
    "    stops = stopwords.words(\"english\") + list(string.punctuation)\n",
    "\n",
    "    # # adding extra stopwords (buy, bought, purchase, purchased)\n",
    "    stops.append('buy')\n",
    "    stops.append('bought')\n",
    "    stops.append('purchase')\n",
    "    stops.append('purchased')\n",
    "    stops.append('product')\n",
    "    stops.append('products')\n",
    "    stops.append('package')\n",
    "    stops.append('packages')\n",
    "\n",
    "    stops.append('quaker')\n",
    "    stops.append('raisin')\n",
    "    stops.append('raisins')\n",
    "    stops.append('bake')\n",
    "    stops.append('baked')\n",
    "    stops.append('oatmeal')\n",
    "    stops.append('dog')\n",
    "    stops.append('dogs')\n",
    "\n",
    "    ## the following codes are from my past nlp project that I use when cleaning the text/ tokens\n",
    "\n",
    "    # special characters\n",
    "    s_chars = '¥₽ÏïŰŬĎŸæ₿₪ÚŇÀèÅ”ĜåŽÖéříÿý€ŝĤ₹áŜŮÂ₴ûÌÇšŘúüëÓ₫ŠčÎŤÆÒœ₩öËäøÍťìĈôàĥÝ¢ç“žðÙÊĉŭÈŒÐÉÔĵùÁů„âÄűĴóêĝÞîØòď฿ČÜþňÛ'\n",
    "    \n",
    "    # Create PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    tokens_no_hashtag = [re.sub(r'#', '', token) for token in tokens]\n",
    "    tokens_no_stopwords = [token.lower() for token in tokens_no_hashtag if token.lower() not in stops]\n",
    "    tokens_no_url = [re.sub(r'http\\S+', '', token) for token in tokens_no_stopwords]\n",
    "    tokens_no_url = [re.sub(r'www\\S+', '', token) for token in tokens_no_url]\n",
    "    tokens_no_special_char = [re.sub(r'[{}]'.format(s_chars), '', token) for token in tokens_no_url]\n",
    "    tokens_no_extra_space = [re.sub(r'\\s\\s+', '', token) for token in tokens_no_special_char]\n",
    "    tokens_alnum = [token for token in tokens_no_extra_space if token.isalnum()]\n",
    "    tokens_stem = [stemmer.stem(token) for token in tokens_alnum]\n",
    "    tokens_final = [token for token in tokens_stem if len(token) > 1]\n",
    "    \n",
    "    return ' '.join(tokens_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which stopwords to remove? Why remove/keep stopwords? Adding in custom stopwords?\n",
    "    - which words we removed\n",
    "    - which words we kept\n",
    "    - which words we added\n",
    "\n",
    "### Stopwords\n",
    "- Stopwords should be removed because we don't want these very common words that are meaningless in our analysis introducing noise and taking up dimensions in our final vectorized matrices.\n",
    "- Stopwords that were removed including all the stopwords in the NLTK corpus, as well as some custom stopwords given the context of this data set. Words like purchase, buy, product, and package were removed because their frequency is likely high due to the data being about food reviews, which makes them relatively meaningless in this context. \n",
    "- Furthermore, since this analysis only focuses on 3 of the top reviewed products from the dataset, and these 3 products all had titles/names that were more than one token long, all but one of those words in the product's name were added to the stopword list (so that only 1 instance of the product being identified in a review could be regex-substituted into a keyword). \n",
    "    - For the Quaker Soft Baked Oatmeal Cookies with Raisin, all relevant naming words other than \"cookie\" were added to the stopword list.\n",
    "    - For the Greenies dog treats, all words except \"Greenies\" were added to the stopword list.\n",
    "    - For the coconut oil, no words were added to the stopword list, and \"coconut oil\" and \"oil\" were considered in the regex substitution section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming vs Lemmatization\n",
    "Since our analysis is focused on counts of words to ultimately do TF-IDF, the part of speech or actual language word doesn't have to be considered. Therefore, stemming was chosen over lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Cleaning and Substitution\n",
    "- Regex substitution was used to change the product name of the products used in this analysis to a keyword specifying which product they were so that we can understand which products were being referenced in a certain review or in our vectorized results.\n",
    "    - \"Cookie\" was used as the identifier for the product with ID \"B007JFMH8M\"\n",
    "    - \"Greenies\" was used as the identifier for the product with ID \"B0026RQTGE\"\n",
    "    - \"Coconut Oil/Oil was used as the identifier for the product with ID \"B003B3OOPA\"\n",
    "- Regex substituion was also used to substitute and clean common words that may appear in a corpus of food reviews, since all of the products we focused on are consumed. These included the words \"tasty\", \"delicious\", \"yummy\", and \"enjoy\" for positive sentiments, and \"nasty\", \"disgusting\", \"rotten\", and \"stale\" for negative ones.\n",
    "- Additionally, regex substitution was used to clean the text:\n",
    "    - Numbers the appear in the corpus were substituted with NUMERIC, since specific numbers don't provide much meaning in an analysis resulting in a vectorized matrix.\n",
    "    - Tokens that contain the same letter three or more times in a row were also removed, since the English language (in almost all cases) outlaws the use of words with 3 or more of the same character in a row, so these erroenous, meaningless words would just add unneeded dimensionality in our final matrices.\n",
    "    - Lastly, regex substituion was used to remove html formatting characters, hastags, URLS, special characters, and extra spaces in the corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram Selection - NEED TO ADD MORE?\n",
    "\n",
    "For this analysis, we chose to use n-grams of size 1 and 2. 1 was chosen so we could find single meaningful words, while n-grams of size 2 were chosen to find common two-word phrases that were meaningful in a TF-IDF analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20983</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A21U4DR8M6I9QN</td>\n",
       "      <td>K. M Merrill \"justine\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1318896000</td>\n",
       "      <td>addictive! but works for night coughing in dogs</td>\n",
       "      <td>my 12 year old sheltie has chronic brochotitis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20984</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A17TDUBB4Z1PEC</td>\n",
       "      <td>jaded_green</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1318550400</td>\n",
       "      <td>genuine Greenies best price</td>\n",
       "      <td>These are genuine Greenies product, not a knoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20985</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>ABQH3WAWMSMBH</td>\n",
       "      <td>tenisbrat87</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1317168000</td>\n",
       "      <td>Perfect for our little doggies</td>\n",
       "      <td>Our dogs love Greenies, but of course, which d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   ProductId          UserId             ProfileName  \\\n",
       "0  20983  B002QWP89S  A21U4DR8M6I9QN  K. M Merrill \"justine\"   \n",
       "1  20984  B002QWP89S  A17TDUBB4Z1PEC             jaded_green   \n",
       "2  20985  B002QWP89S   ABQH3WAWMSMBH             tenisbrat87   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1318896000   \n",
       "1                     1                       1      5  1318550400   \n",
       "2                     1                       1      5  1317168000   \n",
       "\n",
       "                                           Summary  \\\n",
       "0  addictive! but works for night coughing in dogs   \n",
       "1                      genuine Greenies best price   \n",
       "2                   Perfect for our little doggies   \n",
       "\n",
       "                                                Text  \n",
       "0  my 12 year old sheltie has chronic brochotitis...  \n",
       "1  These are genuine Greenies product, not a knoc...  \n",
       "2  Our dogs love Greenies, but of course, which d...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/amazon_fine_foods.csv')\n",
    "# df = pd.read_csv('amazon_fine_foods.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B007JFMH8M    913\n",
       "B0026RQTGE    632\n",
       "B002QWP8H0    632\n",
       "B002QWHJOU    632\n",
       "B002QWP89S    632\n",
       "B003B3OOPA    623\n",
       "B001EO5Q64    567\n",
       "B000VK8AVK    564\n",
       "B007M83302    564\n",
       "B001RVFEP2    564\n",
       "Name: ProductId, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ProductId.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2168, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## limiting our data to just 3 products\n",
    "df = df[df.ProductId.isin(['B007JFMH8M', 'B0026RQTGE', 'B003B3OOPA'])]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the word counts from summary and review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary_tok'] = df.Summary.apply(clean_text)\n",
    "df['text_tok'] = df.Text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(str):\n",
    "    counts = dict()\n",
    "    words = str.split()\n",
    "\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soft</th>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treat</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tast</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yum</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stuff</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hair</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amaz</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chewi</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awesom</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excel</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chew</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wonder</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>healthi</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words\n",
       "love       307\n",
       "great      306\n",
       "good       199\n",
       "soft       174\n",
       "treat       67\n",
       "tast        67\n",
       "best        58\n",
       "price       57\n",
       "yum         48\n",
       "like        45\n",
       "stuff       42\n",
       "hair        41\n",
       "amaz        41\n",
       "chewi       33\n",
       "awesom      33\n",
       "excel       32\n",
       "chew        31\n",
       "use         29\n",
       "wonder      29\n",
       "healthi     29"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary\n",
    "sum_l = df['summary_tok'].astype(str).values.tolist()\n",
    "summary_text = ' '.join(sum_l)\n",
    "summary_df = pd.DataFrame.from_dict(word_count(summary_text), orient='index',columns=['words'])\n",
    "summary_df.sort_values(by = 'words', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>1247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soft</th>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tast</th>\n",
       "      <td>646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hair</th>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tri</th>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realli</th>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treat</th>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skin</th>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also</th>\n",
       "      <td>355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eat</th>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make</th>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teeth</th>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go</th>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>box</th>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>even</th>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>littl</th>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>give</th>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smell</th>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>much</th>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flavor</th>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well</th>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>receiv</th>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cook</th>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>influenst</th>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recommend</th>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kid</th>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snack</th>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazon</th>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>better</th>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           words\n",
       "love        1247\n",
       "use          981\n",
       "soft         785\n",
       "great        764\n",
       "like         749\n",
       "good         720\n",
       "one          658\n",
       "tast         646\n",
       "hair         562\n",
       "tri          557\n",
       "get          508\n",
       "would        408\n",
       "realli       387\n",
       "treat        386\n",
       "skin         379\n",
       "also         355\n",
       "eat          333\n",
       "make         331\n",
       "teeth        317\n",
       "go           313\n",
       "box          309\n",
       "time         293\n",
       "even         291\n",
       "price        288\n",
       "littl        280\n",
       "day          277\n",
       "give         275\n",
       "smell        269\n",
       "much         266\n",
       "flavor       260\n",
       "well         256\n",
       "receiv       247\n",
       "cook         247\n",
       "influenst    245\n",
       "recommend    226\n",
       "kid          224\n",
       "snack        222\n",
       "year         216\n",
       "amazon       202\n",
       "better       201"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review\n",
    "review_l = df['text_tok'].astype(str).values.tolist()\n",
    "review_text = ' '.join(review_l)\n",
    "review_df = pd.DataFrame.from_dict(word_count(review_text), orient='index',columns=['words'])\n",
    "review_df.sort_values(by = 'words', ascending=False).head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newly added stopwords\n",
    "* product/ products\n",
    "* package\n",
    "* want, made, would, could, give, said, say\n",
    "* realli\n",
    "* lol\n",
    "\n",
    "### regex substitution\n",
    "* like/ good/ great/ love/ nice as POSITIVE\n",
    "* bake/ cookie/ oatmeal/ quaker as FOOD_PRODUCT_1\n",
    "* dog/ greenie/ treat as FOOD_PRODUCT_2\n",
    "* coconut oil/ as FOOD_PRODUCT_3\n",
    "* numerical number (170, 20,...) as NUMBER\n",
    "* tasty/ delicious/ flavor as ...\n",
    "* repeating letter words (mmmmmmmm, aaaa. ect)  --> when it repeats more than 3 times, omit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## divide the reviews into positive, netual, and negative\n",
    "def pos_neg(x):\n",
    "    if x  >= 4:\n",
    "        return 'P'\n",
    "    else:\n",
    "        return 'Ng'\n",
    "\n",
    "df['sensetive'] = df.Score.apply(lambda x: pos_neg(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataframe into two parts\n",
    "pos_df = df[df.sensetive == 'P']\n",
    "neg_df = df[df.sensetive == 'Ng']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshaybhide/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/akshaybhide/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "pos_df['text_tok'] = pos_df.Text.apply(clean_text)\n",
    "neg_df['text_tok'] = neg_df.Text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(doc):\n",
    "    # min_df = 0.01, since we wanted to reduce dimensionality and take away words that were not commonly used\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(2, 2), min_df = 0.01)\n",
    "    X = vectorizer.fit_transform(doc) \n",
    "    X = X.toarray()\n",
    "    return pd.DataFrame(X, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_mx = to_matrix(neg_df['text_tok'])\n",
    "pos_mx = to_matrix(pos_df['text_tok'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual caus</th>\n",
       "      <th>actual would</th>\n",
       "      <th>affect stop</th>\n",
       "      <th>also contain</th>\n",
       "      <th>alway lookout</th>\n",
       "      <th>amazon vine</th>\n",
       "      <th>anoth one</th>\n",
       "      <th>ate one</th>\n",
       "      <th>away bewar</th>\n",
       "      <th>bad review</th>\n",
       "      <th>...</th>\n",
       "      <th>would get</th>\n",
       "      <th>would give</th>\n",
       "      <th>would go</th>\n",
       "      <th>would like</th>\n",
       "      <th>would much</th>\n",
       "      <th>would never</th>\n",
       "      <th>would order</th>\n",
       "      <th>would recommend</th>\n",
       "      <th>would take</th>\n",
       "      <th>year old</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.306706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.341275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 322 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     actual caus  actual would  affect stop  also contain  alway lookout  \\\n",
       "0            0.0           0.0          0.0           0.0            0.0   \n",
       "1            0.0           0.0          0.0           0.0            0.0   \n",
       "2            0.0           0.0          0.0           0.0            0.0   \n",
       "3            0.0           0.0          0.0           0.0            0.0   \n",
       "4            0.0           0.0          0.0           0.0            0.0   \n",
       "..           ...           ...          ...           ...            ...   \n",
       "165          0.0           0.0          0.0           0.0            0.0   \n",
       "166          0.0           0.0          0.0           0.0            0.0   \n",
       "167          0.0           0.0          0.0           0.0            0.0   \n",
       "168          0.0           0.0          0.0           0.0            0.0   \n",
       "169          0.0           0.0          0.0           0.0            0.0   \n",
       "\n",
       "     amazon vine  anoth one  ate one  away bewar  bad review  ...  would get  \\\n",
       "0            0.0        0.0      0.0         0.0         0.0  ...        0.0   \n",
       "1            0.0        0.0      0.0         0.0         0.0  ...        0.0   \n",
       "2            0.0        0.0      0.0         0.0         0.0  ...        0.0   \n",
       "3            0.0        0.0      0.0         0.0         0.0  ...        0.0   \n",
       "4            0.0        0.0      0.0         0.0         0.0  ...        0.0   \n",
       "..           ...        ...      ...         ...         ...  ...        ...   \n",
       "165          0.0        0.0      0.0         0.0         0.0  ...        0.0   \n",
       "166          0.0        0.0      0.0         0.0         0.0  ...        0.0   \n",
       "167          0.0        0.0      0.0         0.0         0.0  ...        0.0   \n",
       "168          0.0        0.0      0.0         0.0         0.0  ...        0.0   \n",
       "169          0.0        0.0      0.0         0.0         0.0  ...        0.0   \n",
       "\n",
       "     would give  would go  would like  would much  would never  would order  \\\n",
       "0      0.000000       0.0         0.0         0.0     0.000000          0.0   \n",
       "1      0.000000       0.0         0.0         0.0     0.000000          0.0   \n",
       "2      0.000000       0.0         0.0         0.0     0.000000          0.0   \n",
       "3      0.000000       0.0         0.0         0.0     0.000000          0.0   \n",
       "4      0.000000       0.0         0.0         0.0     0.000000          0.0   \n",
       "..          ...       ...         ...         ...          ...          ...   \n",
       "165    0.000000       0.0         0.0         0.0     0.000000          0.0   \n",
       "166    0.000000       0.0         0.0         0.0     0.000000          0.0   \n",
       "167    0.000000       0.0         0.0         0.0     0.000000          0.0   \n",
       "168    0.306706       0.0         0.0         0.0     0.341275          0.0   \n",
       "169    0.000000       0.0         0.0         0.0     0.000000          0.0   \n",
       "\n",
       "     would recommend  would take  year old  \n",
       "0                0.0         0.5       0.0  \n",
       "1                0.0         0.0       0.0  \n",
       "2                0.0         0.0       0.0  \n",
       "3                0.0         0.0       0.0  \n",
       "4                0.0         0.0       0.0  \n",
       "..               ...         ...       ...  \n",
       "165              0.0         0.0       0.0  \n",
       "166              0.0         0.0       0.0  \n",
       "167              0.0         0.0       0.0  \n",
       "168              0.0         0.0       0.0  \n",
       "169              0.0         0.0       0.0  \n",
       "\n",
       "[170 rows x 322 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolut love</th>\n",
       "      <th>also use</th>\n",
       "      <th>best price</th>\n",
       "      <th>brush teeth</th>\n",
       "      <th>ca wait</th>\n",
       "      <th>clean teeth</th>\n",
       "      <th>definit recommend</th>\n",
       "      <th>dri skin</th>\n",
       "      <th>even better</th>\n",
       "      <th>everi day</th>\n",
       "      <th>...</th>\n",
       "      <th>use skin</th>\n",
       "      <th>vox box</th>\n",
       "      <th>voxbox influenst</th>\n",
       "      <th>whole famili</th>\n",
       "      <th>whole grain</th>\n",
       "      <th>work great</th>\n",
       "      <th>work well</th>\n",
       "      <th>would definit</th>\n",
       "      <th>would recommend</th>\n",
       "      <th>year old</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1998 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      absolut love  also use  best price  brush teeth  ca wait  clean teeth  \\\n",
       "0              0.0       0.0         0.0          0.0      0.0          0.0   \n",
       "1              0.0       0.0         1.0          0.0      0.0          0.0   \n",
       "2              0.0       0.0         0.0          0.0      0.0          0.0   \n",
       "3              0.0       0.0         0.0          0.0      0.0          0.0   \n",
       "4              0.0       0.0         0.0          0.0      0.0          0.0   \n",
       "...            ...       ...         ...          ...      ...          ...   \n",
       "1993           0.0       0.0         0.0          0.0      0.0          0.0   \n",
       "1994           0.0       0.0         0.0          0.0      0.0          0.0   \n",
       "1995           0.0       0.0         0.0          0.0      0.0          0.0   \n",
       "1996           0.0       0.0         0.0          0.0      0.0          0.0   \n",
       "1997           0.0       0.0         0.0          0.0      0.0          0.0   \n",
       "\n",
       "      definit recommend  dri skin  even better  everi day  ...  use skin  \\\n",
       "0                   0.0       0.0          0.0        0.0  ...       0.0   \n",
       "1                   0.0       0.0          0.0        0.0  ...       0.0   \n",
       "2                   0.0       0.0          0.0        0.0  ...       0.0   \n",
       "3                   0.0       0.0          0.0        0.0  ...       0.0   \n",
       "4                   0.0       0.0          0.0        0.0  ...       0.0   \n",
       "...                 ...       ...          ...        ...  ...       ...   \n",
       "1993                0.0       0.0          0.0        0.0  ...       0.0   \n",
       "1994                0.0       0.0          0.0        0.0  ...       0.0   \n",
       "1995                0.0       0.0          0.0        0.0  ...       0.0   \n",
       "1996                0.0       0.0          0.0        0.0  ...       0.0   \n",
       "1997                0.0       0.0          0.0        0.0  ...       0.0   \n",
       "\n",
       "      vox box  voxbox influenst  whole famili  whole grain  work great  \\\n",
       "0         0.0               0.0           0.0          0.0         0.0   \n",
       "1         0.0               0.0           0.0          0.0         0.0   \n",
       "2         0.0               0.0           0.0          0.0         0.0   \n",
       "3         0.0               0.0           0.0          0.0         0.0   \n",
       "4         0.0               0.0           0.0          0.0         0.0   \n",
       "...       ...               ...           ...          ...         ...   \n",
       "1993      0.0               0.0           0.0          0.0         0.0   \n",
       "1994      0.0               0.0           0.0          0.0         0.0   \n",
       "1995      0.0               0.0           0.0          1.0         0.0   \n",
       "1996      0.0               0.0           0.0          0.0         0.0   \n",
       "1997      0.0               0.0           0.0          0.0         0.0   \n",
       "\n",
       "      work well  would definit  would recommend  year old  \n",
       "0           0.0            0.0              0.0       1.0  \n",
       "1           0.0            0.0              0.0       0.0  \n",
       "2           0.0            0.0              0.0       0.0  \n",
       "3           0.0            0.0              0.0       0.0  \n",
       "4           0.0            0.0              0.0       0.0  \n",
       "...         ...            ...              ...       ...  \n",
       "1993        0.0            0.0              0.0       0.0  \n",
       "1994        0.0            0.0              0.0       0.0  \n",
       "1995        0.0            0.0              0.0       0.0  \n",
       "1996        0.0            0.0              0.0       0.0  \n",
       "1997        0.0            0.0              0.0       0.0  \n",
       "\n",
       "[1998 rows x 89 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the TF-idf score for each tokens (n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_neg = neg_mx.T\n",
    "tf_neg['score'] = tf_neg.sum(axis =1)\n",
    "tf_pos = pos_mx.T\n",
    "tf_pos['score'] = tf_pos.sum(axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### top 20 scores for positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>soft chewi</th>\n",
       "      <td>63.364194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love soft</th>\n",
       "      <td>53.399220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use hair</th>\n",
       "      <td>50.059379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year old</th>\n",
       "      <td>49.033671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highli recommend</th>\n",
       "      <td>42.803227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kid love</th>\n",
       "      <td>42.039136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mom voxbox</th>\n",
       "      <td>41.781764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tast like</th>\n",
       "      <td>39.038385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tast great</th>\n",
       "      <td>38.964118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolut love</th>\n",
       "      <td>38.952077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use cook</th>\n",
       "      <td>37.675886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realli like</th>\n",
       "      <td>33.817210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teeth clean</th>\n",
       "      <td>33.496552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also use</th>\n",
       "      <td>30.850444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clean teeth</th>\n",
       "      <td>29.539188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>individu wrap</th>\n",
       "      <td>28.408791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love love</th>\n",
       "      <td>28.084842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realli good</th>\n",
       "      <td>27.730302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use skin</th>\n",
       "      <td>27.621203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love treat</th>\n",
       "      <td>26.994730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      score\n",
       "soft chewi        63.364194\n",
       "love soft         53.399220\n",
       "use hair          50.059379\n",
       "year old          49.033671\n",
       "highli recommend  42.803227\n",
       "kid love          42.039136\n",
       "mom voxbox        41.781764\n",
       "tast like         39.038385\n",
       "tast great        38.964118\n",
       "absolut love      38.952077\n",
       "use cook          37.675886\n",
       "realli like       33.817210\n",
       "teeth clean       33.496552\n",
       "also use          30.850444\n",
       "clean teeth       29.539188\n",
       "individu wrap     28.408791\n",
       "love love         28.084842\n",
       "realli good       27.730302\n",
       "use skin          27.621203\n",
       "love treat        26.994730"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_pos_score = tf_pos[['score']].sort_values(by = 'score',ascending = False).head(20)\n",
    "tf_pos_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### top 20 scores for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>individu wrap</th>\n",
       "      <td>3.359883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year old</th>\n",
       "      <td>3.334901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realli like</th>\n",
       "      <td>2.577729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hair skin</th>\n",
       "      <td>2.474868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kid love</th>\n",
       "      <td>2.448023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glass milk</th>\n",
       "      <td>2.275937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>littl dri</th>\n",
       "      <td>2.201032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plastic jar</th>\n",
       "      <td>2.118957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>throw away</th>\n",
       "      <td>2.056551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>read review</th>\n",
       "      <td>2.056375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soft moist</th>\n",
       "      <td>2.040563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chocol chip</th>\n",
       "      <td>2.015118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also contain</th>\n",
       "      <td>1.988686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soft chewi</th>\n",
       "      <td>1.986921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>look like</th>\n",
       "      <td>1.832907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dental chew</th>\n",
       "      <td>1.825510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>someth would</th>\n",
       "      <td>1.748262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tast good</th>\n",
       "      <td>1.744533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thought would</th>\n",
       "      <td>1.711344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>larg size</th>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>box econom</th>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stuck togeth</th>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like tast</th>\n",
       "      <td>1.692646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>need someth</th>\n",
       "      <td>1.682304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skin hair</th>\n",
       "      <td>1.618752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soft tast</th>\n",
       "      <td>1.605369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ate one</th>\n",
       "      <td>1.601534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mom voxbox</th>\n",
       "      <td>1.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>individu pack</th>\n",
       "      <td>1.552640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tast textur</th>\n",
       "      <td>1.526695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  score\n",
       "individu wrap  3.359883\n",
       "year old       3.334901\n",
       "realli like    2.577729\n",
       "hair skin      2.474868\n",
       "kid love       2.448023\n",
       "glass milk     2.275937\n",
       "littl dri      2.201032\n",
       "plastic jar    2.118957\n",
       "throw away     2.056551\n",
       "read review    2.056375\n",
       "soft moist     2.040563\n",
       "chocol chip    2.015118\n",
       "also contain   1.988686\n",
       "soft chewi     1.986921\n",
       "look like      1.832907\n",
       "dental chew    1.825510\n",
       "someth would   1.748262\n",
       "tast good      1.744533\n",
       "thought would  1.711344\n",
       "larg size      1.707107\n",
       "box econom     1.707107\n",
       "stuck togeth   1.707107\n",
       "like tast      1.692646\n",
       "need someth    1.682304\n",
       "skin hair      1.618752\n",
       "soft tast      1.605369\n",
       "ate one        1.601534\n",
       "mom voxbox     1.577350\n",
       "individu pack  1.552640\n",
       "tast textur    1.526695"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_neg_score = tf_neg[['score']].sort_values(by = 'score',ascending = False).head(30)\n",
    "tf_neg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF report that explains for a business (non-technical) stakeholder:\n",
    "* the features your analysis showed that customers cited as reasons for a poor review\n",
    "* the features your analysis showed that customers cited as reasons for a good review\n",
    "* the most common issues identified from your analysis that generated customer dissatisfaction.\n",
    "\n",
    "Explain to what degree the TF-IDF findings make sense - what are its limitations?\n",
    "\n",
    "- Note from prof: difference between first and third points:\n",
    "- Bullet point 1 is asking you to find the top ngrams for poor reviews after TF-IDF vectorization. Bullet point 3 is asking you for a business analysis explanation that synthesizes the results from 1. It is not a one to one mapping of ngrams to issues. Remember I have said that there is multicollinearity between ngrams generated in vectorization. So just listing the top ngram scores would suffice for 1 but not for 3. You should explain what the pain points are and cite specific examples/action items from the corpus for management to consider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The features your analysis showed that customers cited as reasons for a poor review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The features your analysis showed that customers cited as reasons for a good review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The most common issues identified from your analysis that generated customer dissatisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity and Word Embeddings (2 pts)\n",
    "\n",
    "Using\n",
    "* `TfIdfVectorizer`\n",
    "\n",
    "Identify the most similar pair of reviews from the `amazon-fine-foods.csv` dataset using both Euclidean distance and cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cosine similarity function\n",
    "def similar_doc_cs(vec_df, ori_df):\n",
    "    \"\"\"\n",
    "    Inputs two arguments: vectorized dataframe and the original dataframe\n",
    "    Ouput is the two reviews that had the highest similarity and the score\n",
    "\n",
    "    This function uses the cosine similarity and retrieves the highest value besides 1\n",
    "    \"\"\"\n",
    "    cs_mx = cosine_similarity(vec_df, vec_df)\n",
    "    mod = np.where(cs_mx >= .99, 0, cs_mx)\n",
    "    indices = np.where(mod == mod.max())\n",
    "\n",
    "    count = 0\n",
    "    for n,i in enumerate(mod):\n",
    "        for m,j in enumerate(i):\n",
    "            if j >= mod[indices][0] - 0.001 and n < m and count < 1:\n",
    "                count += 1\n",
    "                print(f'The first review:', '\\n', ori_df.iloc[m,9], '\\n------------------------\\n', \n",
    "            f'The sencond review:', '\\n', ori_df.iloc[n,9])\n",
    "                print(f'The similarity score was {round(j,4)}')\n",
    "\n",
    "\n",
    "## euclidean distance function\n",
    "def similar_doc_ed(vec_df, ori_df):\n",
    "    \"\"\"\n",
    "    Inputs two arguments: vectorized dataframe and the original dataframe\n",
    "    Ouput is the two reviews that had the highest similarity and the score\n",
    "\n",
    "    This function uses the euclidean distance and retrieves the highest value besides 1\n",
    "    \"\"\"\n",
    "    cs_mx = euclidean_distances(vec_df, vec_df)\n",
    "    mod = np.where(cs_mx <= .009, 100, cs_mx)\n",
    "    indices = np.where(mod == mod.min())\n",
    "\n",
    "    count = 0\n",
    "    for n,i in enumerate(mod):\n",
    "        for m,j in enumerate(i):\n",
    "            if j <= mod[indices][0] and n < m and count < 1:\n",
    "                count += 1\n",
    "                print(f'The first review:', '\\n', ori_df.iloc[m,9], '\\n------------------------\\n', \n",
    "            f'The sencond review:', '\\n', ori_df.iloc[n,9])\n",
    "                print(f'The similarity score was {round(j,4)}')\n",
    "                print(n,m)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a dog trainer and have never seen  anything like it....three weeks later,, the beloved sheltie got a bowel blockage from these, use with caution.if the cat gets too many she has the runs....sheltie did better when i upped her thryoid meds, and gave her doggie asthma meds.s\n"
     ]
    }
   ],
   "source": [
    "t = \"I am a dog trainer and have never seen  anything like it....<br /><br />three weeks later,, the beloved sheltie got a bowel blockage from these, use with caution.<br />if the cat gets too many she has the runs....<br />sheltie did better when i upped her thryoid meds, and gave her doggie asthma meds.<br />s\"\n",
    "html = re.compile(r'<.*?>')\n",
    "text = html.sub(r'',t)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review: \n",
      " I loved this product when I initially received it.  I purchased it to use as a dietary supplement and was pleased to discover that it works wonders on hair and skin as well.  I read the label carefully because I was concerned about whether or not to refrigerate.  The label clearly said that refrigeration was not required.  However, after appx. 2 weeks of using as a daily dietary supplement, I began to notice an awful cramping, nauseous feeling not long after I took it.  It was the sickest I have ever felt in my entire life and I repeated it just to be sure that it was the coconut oil causing it and not something else I had eaten.  Same thing again.  All I can figure is the oil went rancid.  There's no other explanation for why it suddenly started causing me to be sick.  So, word to the wise, consider refrigerating this product.  I've stopped using it altogether because I now associate the smell of it with sickness. :( \n",
      "------------------------\n",
      " The sencond review: \n",
      " I tried this for my hair and skin.  It didn't absorb into either one.  I wore it on my hair for three hours and had to wash my hair three times to get it out.  I had it on my skin for three or four hours also and it never absorbed in, I had to wipe it off.  I didn't see a difference in my hair or skin afterwards either.  Disappointed to say the least.\n",
      "The similarity score was 0.7859\n"
     ]
    }
   ],
   "source": [
    "similar_doc_cs(neg_mx, neg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review: \n",
      " I loved this product when I initially received it.  I purchased it to use as a dietary supplement and was pleased to discover that it works wonders on hair and skin as well.  I read the label carefully because I was concerned about whether or not to refrigerate.  The label clearly said that refrigeration was not required.  However, after appx. 2 weeks of using as a daily dietary supplement, I began to notice an awful cramping, nauseous feeling not long after I took it.  It was the sickest I have ever felt in my entire life and I repeated it just to be sure that it was the coconut oil causing it and not something else I had eaten.  Same thing again.  All I can figure is the oil went rancid.  There's no other explanation for why it suddenly started causing me to be sick.  So, word to the wise, consider refrigerating this product.  I've stopped using it altogether because I now associate the smell of it with sickness. :( \n",
      "------------------------\n",
      " The sencond review: \n",
      " I tried this for my hair and skin.  It didn't absorb into either one.  I wore it on my hair for three hours and had to wash my hair three times to get it out.  I had it on my skin for three or four hours also and it never absorbed in, I had to wipe it off.  I didn't see a difference in my hair or skin afterwards either.  Disappointed to say the least.\n",
      "The similarity score was 0.6544\n",
      "68 76\n"
     ]
    }
   ],
   "source": [
    "similar_doc_ed(neg_mx, neg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review: \n",
      " I bought a jar of this thing on Aug 16 for seven bucks (and free delivery if over Amazon's minimum for such). It came pretty fast, in a semi-liquid state but not leaking; rather well packaged. I let it melt completely in a warm room (CO melts at about 80 deg F) and then put it in the fridge to let it solidify. One of the reasons I did (and always do) that is to smooth out the texture: as is, every type of CO I've tried comes in like a bunch of white fibers stuck in more even liquid, but if you thaw it completely and then freeze, it becomes all evenly semi-transparent, like one thick candle sort of thing. It probably doesn't matter; just a personal quirk, I guess.<br /><br />Anyway, this is good stuff: the taste is nice -- very good for sweetich things like carrots or plums, for example (I always have a bowl of very lightly steamed carrots around, and I snack on them, taking one and dipping it into CO, which is good for two reasons: first, it tastes great, and second, carotene is fat-soluble so you need to combine carrots with some sort of fat or it won't be used by the body well).<br /><br />All in all, there isn't much to say -- CO is CO is CO; this kind is no different from other types I've tried. CO in general is shamelessly hyped as a sort of panacea from every health problem known to man; this is mostly lies, but two things are true: it is tasty, and it's good for skin/hair (I've tried both, and it's true, it works). Don't believe anything else and don't \"take\" it daily as if it were a medicine: it's pure saturated fat and that's the end of it: it will raise your cholesterol as any other kind of saturated fat. As far as its being a weight-loss agent, this is not a complete lie, but remember, it works only if you _substitute_ CO for other kinds of saturated fats (that means quit eating butter, meat, etc.) If, otoh, you simply _add_ it to your normal diet, you're not gonna lose any weight -- instead, you're gonna gain weight at least as good as you would otherwise + increase your cholesterol. But if you decide to substitute, do not think it's an easy choice: CO is pure saturated fat, whereas other sources of saturated fats (butter, meat, etc.) come with a lot of nutritional goodness like vitamins, proteins, and the like -- things that CO is wholly devoid of. So my suggestion, for what it's worth (not much, I know) is to use CO just as another agreeable food to be used in moderation, not a magic medicine. I read somewhere that a 40-sh male in good health on a 2000 cal/day diet can get 200 calories from saturated fats w/o harming himself.<br /><br />One other thing I wanted to mention: the jar says it's \"pure extra virgin\" -- keep in mind that \"extra virgin\" is meaningful only when applied to olive oil. That is because there are specific and stringent industry standards stipulating what constitutes \"virgin\", \"extra virgin\", etc. There are no such standards about any other oils, so saying your CO is \"extra virgin\" communicates zero useful information and is no more than disingenuous hand-waving. It's like Nutiva says on their jars \"no cholesterol!\". No kidding, it's like wood-free nails. Of course plant-derived foods have no cholesterol, but BS rules in this industry, so just be aware of what's what and smart enough to dismiss useless hype like claims of a CO being \"extra virgin\".<br /><br />OK, there's really no more to say: as long as you use this tasty substance rationally, it's good stuff, so bottomline is: recommended -- except for one thing: just as I was deciding it's time to post a review and navigating to this page, I noticed that the price went up by the mind-blowing 60 percent since, not just since I bought it (two weeks ago), but literally within the last few days. Does my salary go up 60 percent in a couple of days? No. So, while the product is good and I can't give it less than five stars, I will suggest you do not reward naked greed and at least try to find better-priced alternatives. Especially in this economic environment that we have right now. The current price here is even higher than it is in Wegman's (brick-and-mortar grocery store; they carry it, but it's more than the seven bucks I bought it for here).<br /><br />PS. Btw, I see now someone reports that this oil comes in a glass jar: this is a bare-faced lie. The jar is transparent plastic, quite soft actually, so you can't _honestly_ mistake it for glass. Be extremely suspicious about these massive five-star reviews (here and elsewhere); I for a long time have a feeling those are posted by manufacturers' employees to create hype (and why not? Amazon does nothing to guard against fake reviews; all one needs to do in order to start posting reviews (real OR fake) is buy something once from this site). Some reviews read like the reviewer hasn't actually seen the product in question and is interested only in adding another five-star rating, not the review.<br /><br />PPS. Thanks to commenters Lulu and Adriana for additional info and helpful suggestions. \n",
      "------------------------\n",
      " The sencond review: \n",
      " I have tried a few different extra virgin coconut oil products in the past (Vitamin shoppe brand, parachute,etc) and they tend to have a grainy texture to them. Not a major problem, but who wants to pick shredded coconut out of their hair? This brand was so silky, smooth, and smelled wonderful. I think I found my favorite brand of EVCO!\n",
      "The similarity score was 0.9863\n"
     ]
    }
   ],
   "source": [
    "similar_doc_cs(pos_mx, pos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.41421356, 1.41421356, ..., 1.41421356, 1.41421356,\n",
       "        1.41421356],\n",
       "       [1.41421356, 0.        , 1.41421356, ..., 1.41421356, 1.41421356,\n",
       "        1.41421356],\n",
       "       [1.41421356, 1.41421356, 0.        , ..., 1.41421356, 1.41421356,\n",
       "        1.41421356],\n",
       "       ...,\n",
       "       [1.41421356, 1.41421356, 1.41421356, ..., 0.        , 1.41421356,\n",
       "        1.41421356],\n",
       "       [1.41421356, 1.41421356, 1.41421356, ..., 1.41421356, 0.        ,\n",
       "        0.96628938],\n",
       "       [1.41421356, 1.41421356, 1.41421356, ..., 1.41421356, 0.96628938,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_mx = euclidean_distances(pos_mx, pos_mx)\n",
    "cs_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review: \n",
      " I bought a jar of this thing on Aug 16 for seven bucks (and free delivery if over Amazon's minimum for such). It came pretty fast, in a semi-liquid state but not leaking; rather well packaged. I let it melt completely in a warm room (CO melts at about 80 deg F) and then put it in the fridge to let it solidify. One of the reasons I did (and always do) that is to smooth out the texture: as is, every type of CO I've tried comes in like a bunch of white fibers stuck in more even liquid, but if you thaw it completely and then freeze, it becomes all evenly semi-transparent, like one thick candle sort of thing. It probably doesn't matter; just a personal quirk, I guess.<br /><br />Anyway, this is good stuff: the taste is nice -- very good for sweetich things like carrots or plums, for example (I always have a bowl of very lightly steamed carrots around, and I snack on them, taking one and dipping it into CO, which is good for two reasons: first, it tastes great, and second, carotene is fat-soluble so you need to combine carrots with some sort of fat or it won't be used by the body well).<br /><br />All in all, there isn't much to say -- CO is CO is CO; this kind is no different from other types I've tried. CO in general is shamelessly hyped as a sort of panacea from every health problem known to man; this is mostly lies, but two things are true: it is tasty, and it's good for skin/hair (I've tried both, and it's true, it works). Don't believe anything else and don't \"take\" it daily as if it were a medicine: it's pure saturated fat and that's the end of it: it will raise your cholesterol as any other kind of saturated fat. As far as its being a weight-loss agent, this is not a complete lie, but remember, it works only if you _substitute_ CO for other kinds of saturated fats (that means quit eating butter, meat, etc.) If, otoh, you simply _add_ it to your normal diet, you're not gonna lose any weight -- instead, you're gonna gain weight at least as good as you would otherwise + increase your cholesterol. But if you decide to substitute, do not think it's an easy choice: CO is pure saturated fat, whereas other sources of saturated fats (butter, meat, etc.) come with a lot of nutritional goodness like vitamins, proteins, and the like -- things that CO is wholly devoid of. So my suggestion, for what it's worth (not much, I know) is to use CO just as another agreeable food to be used in moderation, not a magic medicine. I read somewhere that a 40-sh male in good health on a 2000 cal/day diet can get 200 calories from saturated fats w/o harming himself.<br /><br />One other thing I wanted to mention: the jar says it's \"pure extra virgin\" -- keep in mind that \"extra virgin\" is meaningful only when applied to olive oil. That is because there are specific and stringent industry standards stipulating what constitutes \"virgin\", \"extra virgin\", etc. There are no such standards about any other oils, so saying your CO is \"extra virgin\" communicates zero useful information and is no more than disingenuous hand-waving. It's like Nutiva says on their jars \"no cholesterol!\". No kidding, it's like wood-free nails. Of course plant-derived foods have no cholesterol, but BS rules in this industry, so just be aware of what's what and smart enough to dismiss useless hype like claims of a CO being \"extra virgin\".<br /><br />OK, there's really no more to say: as long as you use this tasty substance rationally, it's good stuff, so bottomline is: recommended -- except for one thing: just as I was deciding it's time to post a review and navigating to this page, I noticed that the price went up by the mind-blowing 60 percent since, not just since I bought it (two weeks ago), but literally within the last few days. Does my salary go up 60 percent in a couple of days? No. So, while the product is good and I can't give it less than five stars, I will suggest you do not reward naked greed and at least try to find better-priced alternatives. Especially in this economic environment that we have right now. The current price here is even higher than it is in Wegman's (brick-and-mortar grocery store; they carry it, but it's more than the seven bucks I bought it for here).<br /><br />PS. Btw, I see now someone reports that this oil comes in a glass jar: this is a bare-faced lie. The jar is transparent plastic, quite soft actually, so you can't _honestly_ mistake it for glass. Be extremely suspicious about these massive five-star reviews (here and elsewhere); I for a long time have a feeling those are posted by manufacturers' employees to create hype (and why not? Amazon does nothing to guard against fake reviews; all one needs to do in order to start posting reviews (real OR fake) is buy something once from this site). Some reviews read like the reviewer hasn't actually seen the product in question and is interested only in adding another five-star rating, not the review.<br /><br />PPS. Thanks to commenters Lulu and Adriana for additional info and helpful suggestions. \n",
      "------------------------\n",
      " The sencond review: \n",
      " I have tried a few different extra virgin coconut oil products in the past (Vitamin shoppe brand, parachute,etc) and they tend to have a grainy texture to them. Not a major problem, but who wants to pick shredded coconut out of their hair? This brand was so silky, smooth, and smelled wonderful. I think I found my favorite brand of EVCO!\n",
      "The similarity score was 0.1657\n",
      "576 872\n"
     ]
    }
   ],
   "source": [
    "similar_doc_ed(pos_mx, pos_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (3pts)\n",
    "\n",
    "You are an NLP data scientist working at Fandango. You observe the following dataset in your review comments:\n",
    "\n",
    "**Intent to Buy Tickets:**\n",
    "1.\tLove this movie. Can’t wait!\n",
    "2.\tI want to see this movie so bad.\n",
    "3.\tThis movie looks amazing.\n",
    "\n",
    "**No Intent to Buy Tickets:**\n",
    "1.\tLooks bad.\n",
    "2.\tHard pass to see this bad movie.\n",
    "3.\tSo boring!\n",
    "\n",
    "You can consider the following stopwords for removal: `to`, `this`.\n",
    "\n",
    "Is the following review an `Intent to Buy` or `No Intent to Buy`? Show your work for each computation.\n",
    "> This looks so bad.\n",
    "\n",
    "You'll need to compute:\n",
    "* Prior\n",
    "* Likelihood\n",
    "* Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prior:**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(y = Intent) &= 1/2   \\\\\n",
    "P(y = No Intent) &= 1/2 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "**Likelihood:**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(x \\mid y = Intent) & = P(x = \"looks\" \\mid y = Intent) * P(x = \"so\" \\mid y = Intent) * P(x = \"bad\" \\mid y = Intent)   \\\\\n",
    "& = (1/3) * (1/3) * (1/3) \\\\\n",
    "& = 1/3 * 1/3 * 1/3 \\\\\n",
    "& = 1/27 \\\\\n",
    "\\\\\n",
    "\\\\\n",
    "P(x \\mid y = No Intent) & = P(x = \"looks\" \\mid y = No Intent) * P(x = \"so\" \\mid y = No Intent) * P(x = \"bad\" \\mid y = No Intent)   \\\\\n",
    "& = (1/3) * (1/3) * (2/3) \\\\\n",
    "& = 1/3 * 1/3 * 2/3 \\\\\n",
    "& = 2/27 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "**Evidence:**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(x) &= P(x \\mid y = Intent)*P(y = Intent) + P(x \\mid y = No Intent)*P(y = No Intent)   \\\\\n",
    "& = (1/27) * (1/2) + (2/27) * (1/2) \\\\\n",
    "& = 1/18 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "**Posterior:**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(y = Intent \\mid x) &= (P(x \\mid y = Intent) * P(y = Intent))/ P(x)  \\\\\n",
    "& = ((1/27) * (1/2)) / (1/18) \\\\\n",
    "& = 1/3 \\\\\n",
    "\\\\\n",
    "\\\\\n",
    "P(y = No Intent \\mid x) &= (P(x \\mid y = No Intent) * P(y = No Intent))/ P(x) \\\\\n",
    "& = ((2/27) * (1/2)) / (1/18) \\\\\n",
    "& = 2/3 \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the posterior probability,  \"This looks so bad.\" will be classified as No Intent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
