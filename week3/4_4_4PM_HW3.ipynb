{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3\n",
    "\n",
    "Submit via Slack. Due on **Tuesday, April 12th, 2022, 6:29pm PST**. You may work with one other person.\n",
    "## TF-IDF (5pts)\n",
    "\n",
    "You are an analyst working for Amazon's product team, and charged with identifying areas for improvement for the toy reviews.\n",
    "\n",
    "Using the **amazon-fine-foods.csv** dataset, clean and parse the text reviews. Explain the decisions you make:\n",
    "- why remove/keep stopwords?\n",
    "- which stopwords to remove?\n",
    "- stemming versus lemmatization?\n",
    "- regex cleaning and substitution?\n",
    "- adding in custom stopwords?\n",
    "- what `n` for your `n-grams`?\n",
    "\n",
    "Finally, generate a TF-IDF report that explains for a business (non-technical) stakeholder:\n",
    "* the features your analysis showed that customers cited as reasons for a poor review\n",
    "* the features your analysis showed that customers cited as reasons for a good review\n",
    "* the most common issues identified from your analysis that generated customer dissatisfaction.\n",
    "\n",
    "Explain to what degree the TF-IDF findings make sense - what are its limitations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize text into words. Convert texts to lower case.\n",
    "    Remove hashtags, punctuations, stopwords, website links, extra spaces, non-alphanumeric characters and \n",
    "    single character. stemtize texts.\n",
    "    \"\"\"\n",
    "\n",
    "    # erase html language characters\n",
    "    html = re.compile(r'<.*?>')\n",
    "    text = html.sub(r'',text)\n",
    "\n",
    "    # year phrases\n",
    "    text = re.sub(r'(\\-?yrs?)', ' year', text)\n",
    "    text = re.sub(r'(\\-?years?\\-?olds?)', ' year old', text)\n",
    "\n",
    "    # birthday\n",
    "    text = re.sub(r'([Bb]\\-?[Dd]ays?)', 'birthday', text)\n",
    "\n",
    "    # holiday words\n",
    "    text = re.sub(r'([Xx][Mm]as|[Cc]hrist\\-[Mm]as)', 'christmas', text)\n",
    "    text = re.sub(r'([Nn]ew\\-[Yy]ears?)', 'new years', text)\n",
    "\n",
    "    tokens = [token for token in nltk.word_tokenize(text)]\n",
    "    \n",
    "    # Combine stopwords and punctuation\n",
    "    stops = stopwords.words(\"english\") + list(string.punctuation)\n",
    "\n",
    "    # # adding extra stopwords (buy, bought, purchase, purchased)\n",
    "    stops.append('buy')\n",
    "    stops.append('bought')\n",
    "    stops.append('purchase')\n",
    "    stops.append('purchased')\n",
    "\n",
    "    ## the following codes are from my past nlp project that I use when cleaning the text/ tokens\n",
    "\n",
    "    # special characters\n",
    "    s_chars = '¥₽ÏïŰŬĎŸæ₿₪ÚŇÀèÅ”ĜåŽÖéříÿý€ŝĤ₹áŜŮÂ₴ûÌÇšŘúüëÓ₫ŠčÎŤÆÒœ₩öËäøÍťìĈôàĥÝ¢ç“žðÙÊĉŭÈŒÐÉÔĵùÁů„âÄűĴóêĝÞîØòď฿ČÜþňÛ'\n",
    "    \n",
    "    # Create PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    tokens_no_hashtag = [re.sub(r'#', '', token) for token in tokens]\n",
    "    tokens_no_stopwords = [token.lower() for token in tokens_no_hashtag if token.lower() not in stops]\n",
    "    tokens_no_url = [re.sub(r'http\\S+', '', token) for token in tokens_no_stopwords]\n",
    "    tokens_no_url = [re.sub(r'www\\S+', '', token) for token in tokens_no_url]\n",
    "    tokens_no_special_char = [re.sub(r'[{}]'.format(s_chars), '', token) for token in tokens_no_url]\n",
    "    tokens_no_extra_space = [re.sub(r'\\s\\s+', '', token) for token in tokens_no_special_char]\n",
    "    tokens_alnum = [token for token in tokens_no_extra_space if token.isalnum()]\n",
    "    tokens_stem = [stemmer.stem(token) for token in tokens_alnum]\n",
    "    tokens_final = [token for token in tokens_stem if len(token) > 1]\n",
    "    \n",
    "    return ' '.join(tokens_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which stopwords to remove? Why remove/keep stopwords? Adding in custom stopwords?\n",
    "    - which words we removed\n",
    "    - which words we kept\n",
    "    - which words we added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stemming versus lemmatization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- regex cleaning and substitution?\n",
    "    - cleaned html format strings\n",
    "    - cleaned phrases like `year`, `birthday`, and holiday words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- what `n` for your `n-grams`?\n",
    "    - we chose n = 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20983</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A21U4DR8M6I9QN</td>\n",
       "      <td>K. M Merrill \"justine\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1318896000</td>\n",
       "      <td>addictive! but works for night coughing in dogs</td>\n",
       "      <td>my 12 year old sheltie has chronic brochotitis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20984</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A17TDUBB4Z1PEC</td>\n",
       "      <td>jaded_green</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1318550400</td>\n",
       "      <td>genuine Greenies best price</td>\n",
       "      <td>These are genuine Greenies product, not a knoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20985</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>ABQH3WAWMSMBH</td>\n",
       "      <td>tenisbrat87</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1317168000</td>\n",
       "      <td>Perfect for our little doggies</td>\n",
       "      <td>Our dogs love Greenies, but of course, which d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   ProductId          UserId             ProfileName  \\\n",
       "0  20983  B002QWP89S  A21U4DR8M6I9QN  K. M Merrill \"justine\"   \n",
       "1  20984  B002QWP89S  A17TDUBB4Z1PEC             jaded_green   \n",
       "2  20985  B002QWP89S   ABQH3WAWMSMBH             tenisbrat87   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1318896000   \n",
       "1                     1                       1      5  1318550400   \n",
       "2                     1                       1      5  1317168000   \n",
       "\n",
       "                                           Summary  \\\n",
       "0  addictive! but works for night coughing in dogs   \n",
       "1                      genuine Greenies best price   \n",
       "2                   Perfect for our little doggies   \n",
       "\n",
       "                                                Text  \n",
       "0  my 12 year old sheltie has chronic brochotitis...  \n",
       "1  These are genuine Greenies product, not a knoc...  \n",
       "2  Our dogs love Greenies, but of course, which d...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('amazon_fine_foods.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B007JFMH8M    913\n",
       "B0026RQTGE    632\n",
       "B002QWP8H0    632\n",
       "B002QWHJOU    632\n",
       "B002QWP89S    632\n",
       "B003B3OOPA    623\n",
       "B001EO5Q64    567\n",
       "B000VK8AVK    564\n",
       "B007M83302    564\n",
       "B001RVFEP2    564\n",
       "Name: ProductId, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ProductId.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2177, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## limiting our data to just 3 products\n",
    "df = df[df.ProductId.isin(['B007JFMH8M', 'B0026RQTGE', 'B002QWP8H0'])]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## divide the reviews into positive, netual, and negative\n",
    "def pos_neg(x):\n",
    "    if x  >= 4:\n",
    "        return 'P'\n",
    "    else:\n",
    "        return 'Ng'\n",
    "\n",
    "df['sensetive'] = df.Score.apply(lambda x: pos_neg(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataframe into two parts\n",
    "pos_df = df[df.sensetive == 'P']\n",
    "neg_df = df[df.sensetive == 'Ng']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamato0615/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/yamato0615/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "pos_df['text_tok'] = pos_df.Text.apply(clean_text)\n",
    "neg_df['text_tok'] = neg_df.Text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(doc):\n",
    "    # min_df = 0.01, since we wanted to reduce dimensionality and take away words that were not commonly used\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df = 0.01)\n",
    "    X = vectorizer.fit_transform(doc) \n",
    "    X = X.toarray()\n",
    "    return pd.DataFrame(X, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamato0615/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "neg_mx = to_matrix(neg_df['text_tok'])\n",
    "pos_mx = to_matrix(pos_df['text_tok'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>10 year</th>\n",
       "      <th>100</th>\n",
       "      <th>100 digest</th>\n",
       "      <th>100lb</th>\n",
       "      <th>100lb plu</th>\n",
       "      <th>12</th>\n",
       "      <th>12g</th>\n",
       "      <th>12g sugar</th>\n",
       "      <th>12hr</th>\n",
       "      <th>...</th>\n",
       "      <th>yorki 18</th>\n",
       "      <th>yorki breed</th>\n",
       "      <th>yorkshir</th>\n",
       "      <th>yorkshir terrier</th>\n",
       "      <th>younger</th>\n",
       "      <th>younger dog</th>\n",
       "      <th>younger saint</th>\n",
       "      <th>yummi</th>\n",
       "      <th>ziploc</th>\n",
       "      <th>ziploc top</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.169433</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 3801 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      10  10 year  100  100 digest  100lb  100lb plu   12  12g  12g sugar  \\\n",
       "0    0.0      0.0  0.0         0.0    0.0        0.0  0.0  0.0        0.0   \n",
       "1    0.0      0.0  0.0         0.0    0.0        0.0  0.0  0.0        0.0   \n",
       "2    0.0      0.0  0.0         0.0    0.0        0.0  0.0  0.0        0.0   \n",
       "3    0.0      0.0  0.0         0.0    0.0        0.0  0.0  0.0        0.0   \n",
       "4    0.0      0.0  0.0         0.0    0.0        0.0  0.0  0.0        0.0   \n",
       "..   ...      ...  ...         ...    ...        ...  ...  ...        ...   \n",
       "187  0.0      0.0  0.0         0.0    0.0        0.0  0.0  0.0        0.0   \n",
       "188  0.0      0.0  0.0         0.0    0.0        0.0  0.0  0.0        0.0   \n",
       "189  0.0      0.0  0.0         0.0    0.0        0.0  0.0  0.0        0.0   \n",
       "190  0.0      0.0  0.0         0.0    0.0        0.0  0.0  0.0        0.0   \n",
       "191  0.0      0.0  0.0         0.0    0.0        0.0  0.0  0.0        0.0   \n",
       "\n",
       "     12hr  ...  yorki 18  yorki breed  yorkshir  yorkshir terrier  younger  \\\n",
       "0     0.0  ...       0.0          0.0       0.0               0.0      0.0   \n",
       "1     0.0  ...       0.0          0.0       0.0               0.0      0.0   \n",
       "2     0.0  ...       0.0          0.0       0.0               0.0      0.0   \n",
       "3     0.0  ...       0.0          0.0       0.0               0.0      0.0   \n",
       "4     0.0  ...       0.0          0.0       0.0               0.0      0.0   \n",
       "..    ...  ...       ...          ...       ...               ...      ...   \n",
       "187   0.0  ...       0.0          0.0       0.0               0.0      0.0   \n",
       "188   0.0  ...       0.0          0.0       0.0               0.0      0.0   \n",
       "189   0.0  ...       0.0          0.0       0.0               0.0      0.0   \n",
       "190   0.0  ...       0.0          0.0       0.0               0.0      0.0   \n",
       "191   0.0  ...       0.0          0.0       0.0               0.0      0.0   \n",
       "\n",
       "     younger dog  younger saint     yummi  ziploc  ziploc top  \n",
       "0            0.0            0.0  0.000000     0.0         0.0  \n",
       "1            0.0            0.0  0.000000     0.0         0.0  \n",
       "2            0.0            0.0  0.000000     0.0         0.0  \n",
       "3            0.0            0.0  0.000000     0.0         0.0  \n",
       "4            0.0            0.0  0.000000     0.0         0.0  \n",
       "..           ...            ...       ...     ...         ...  \n",
       "187          0.0            0.0  0.169433     0.0         0.0  \n",
       "188          0.0            0.0  0.000000     0.0         0.0  \n",
       "189          0.0            0.0  0.000000     0.0         0.0  \n",
       "190          0.0            0.0  0.000000     0.0         0.0  \n",
       "191          0.0            0.0  0.000000     0.0         0.0  \n",
       "\n",
       "[192 rows x 3801 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF report that explains for a business (non-technical) stakeholder:\n",
    "* the features your analysis showed that customers cited as reasons for a poor review\n",
    "* the features your analysis showed that customers cited as reasons for a good review\n",
    "* the most common issues identified from your analysis that generated customer dissatisfaction.\n",
    "\n",
    "Explain to what degree the TF-IDF findings make sense - what are its limitations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The features your analysis showed that customers cited as reasons for a poor review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The features your analysis showed that customers cited as reasons for a good review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The most common issues identified from your analysis that generated customer dissatisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity and Word Embeddings (2 pts)\n",
    "\n",
    "Using\n",
    "* `TfIdfVectorizer`\n",
    "\n",
    "Identify the most similar pair of reviews from the `amazon-fine-foods.csv` dataset using both Euclidean distance and cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cosine similarity function\n",
    "def similar_doc_cs(vec_df, ori_df):\n",
    "    \"\"\"\n",
    "    Inputs two arguments: vectorized dataframe and the original dataframe\n",
    "    Ouput is the two reviews that had the highest similarity and the score\n",
    "\n",
    "    This function uses the cosine similarity and retrieves the highest value besides 1\n",
    "    \"\"\"\n",
    "    cs_mx = cosine_similarity(vec_df, vec_df)\n",
    "    mod = np.where(cs_mx >= .99, 0, cs_mx)\n",
    "    indices = np.where(mod == mod.max())\n",
    "\n",
    "    count = 0\n",
    "    for n,i in enumerate(mod):\n",
    "        for m,j in enumerate(i):\n",
    "            if j >= mod[indices][0] - 0.001 and n < m and count < 1:\n",
    "                count += 1\n",
    "                print(f'The first review:', '\\n', ori_df.iloc[m,9], '\\n------------------------\\n', \n",
    "            f'The sencond review:', '\\n', ori_df.iloc[n,9])\n",
    "                print(f'The similarity score was {round(j,4)}')\n",
    "\n",
    "\n",
    "## euclidean distance function\n",
    "def similar_doc_ed(vec_df, ori_df):\n",
    "    \"\"\"\n",
    "    Inputs two arguments: vectorized dataframe and the original dataframe\n",
    "    Ouput is the two reviews that had the highest similarity and the score\n",
    "\n",
    "    This function uses the euclidean distance and retrieves the highest value besides 1\n",
    "    \"\"\"\n",
    "    cs_mx = euclidean_distances(vec_df, vec_df)\n",
    "    mod = np.where(cs_mx <= .009, 100, cs_mx)\n",
    "    indices = np.where(mod == mod.min())\n",
    "\n",
    "    count = 0\n",
    "    for n,i in enumerate(mod):\n",
    "        for m,j in enumerate(i):\n",
    "            if j <= mod[indices][0] and n < m and count < 1:\n",
    "                count += 1\n",
    "                print(f'The first review:', '\\n', ori_df.iloc[m,9], '\\n------------------------\\n', \n",
    "            f'The sencond review:', '\\n', ori_df.iloc[n,9])\n",
    "                print(f'The similarity score was {round(j,4)}')\n",
    "                print(n,m)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a dog trainer and have never seen  anything like it....three weeks later,, the beloved sheltie got a bowel blockage from these, use with caution.if the cat gets too many she has the runs....sheltie did better when i upped her thryoid meds, and gave her doggie asthma meds.s\n"
     ]
    }
   ],
   "source": [
    "t = \"I am a dog trainer and have never seen  anything like it....<br /><br />three weeks later,, the beloved sheltie got a bowel blockage from these, use with caution.<br />if the cat gets too many she has the runs....<br />sheltie did better when i upped her thryoid meds, and gave her doggie asthma meds.<br />s\"\n",
    "html = re.compile(r'<.*?>')\n",
    "text = html.sub(r'',t)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review: \n",
      " I like oatmeal and raisin cookies but I don't eat them very often.  That's probably because my wife makes such awesome chocolate cookies.  When I do get an opportunity to try an oatmeal and raisin cookie it is usually from a store-bought package.  I usually come away impressed with how moist these pre-packaged cookies are.  That's what surprised me most about Quaker Soft Baked Oatmeal Cookie;  They're soft but they aren't moist.  There is a distinct soft dryness to the cookies. The taste was good and my grand daughter quickly ate up most of them.  She liked them because they are tasty and soft.  I agree and I guess two out of three criteria is good but I was looking for one more quality \n",
      "------------------------\n",
      " The sencond review: \n",
      " I received the oatmeal raisin cookie in my momvoxbox from influenster and it was pleasantly surprised by how chewy and tasty the cookie was considering that oatmeal raisin cookies are usually hard.  I'm not a big fan of the oatmeal raisin cookies but my little brother loved it and i'll most likely purchase the chocolate chip cookies because their a healthier alternative to most i think :)\n",
      "The similarity score was 0.489\n"
     ]
    }
   ],
   "source": [
    "similar_doc_cs(neg_mx, neg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review: \n",
      " I will continue to give these to my dogs. They like them and supposedly they clean their teeth.<br />I started to give these to my dog daily, but her digestive system did not like it enough for that. Most of it seems to be digestible, but she threw up green foam on the morning after the second day in a row of eating one-a-day greenies. Note that she was also constipated as well. Greenie advises to have plenty of water available for drinking these, so I am guess they know these side effects can occur. My dog has plenty of water available, but she didn't drink any more than normal, which may have resulted in the effects. I don't plan on training my dog to drink lots of extra water after eating these. Giving here one every now and then shows no negative effects, and that what I plan on doing.<br />Not a complaint to me, but I think people who like giving there dog all natural health food should know that this product is far from \"green.\" It is pumped pull of artificial ingredients like a multivitamin. It is also made from concentrated grain protein, rather than the animal protein dogs should be getting. I personally don't feel that this is that big of a deal health wise for a creature than lives 12-18 years, but or people who invest the extra money on organic/natural dog food, you should be staying away from this product. \n",
      "------------------------\n",
      " The sencond review: \n",
      " Half way through the supply, I noticed that there were tiny white worms boring through a greenie so I checked the remaining greenies and they also had worms.  Please empty the contents of the box and check the greenies when you receive your order before giving them to your dog.  This may have been an isolated incident but it would only take a few minutes to check the contents for freshness.\n",
      "The similarity score was 1.3838\n",
      "0 1\n"
     ]
    }
   ],
   "source": [
    "similar_doc_ed(neg_mx, neg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review: \n",
      " Greenies have been a part of my dogs lives for nearly thirteen years and they look forward to one everyday.  I ordered them from Amazon and everything went smoothly.  I did not have a hitch in the order or length of time getting them. Thank you so much, Linda Turner  I would recommend ordering these to anyone. \n",
      "------------------------\n",
      " The sencond review: \n",
      " My dog is goofy enough but when he knows one of these are comming,  he really goes nuts.  I did not have a hitch in the order or length of time getting them. Thank you so much, Linda Turner I would recommend ordering these to anyone. Greenies have been a part of my dogs lives for nearly thirteen years and they look forward to one everyday. I ordered them from Amazon and everything went smoothly.\n",
      "The similarity score was 0.9183\n"
     ]
    }
   ],
   "source": [
    "similar_doc_cs(pos_mx, pos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.10734243e-08, 1.39583802e+00, 1.35772903e+00, ...,\n",
       "        1.41421356e+00, 1.41421356e+00, 1.36594777e+00],\n",
       "       [1.39583802e+00, 1.49011612e-08, 1.26984116e+00, ...,\n",
       "        1.35567183e+00, 1.40962530e+00, 1.41421356e+00],\n",
       "       [1.35772903e+00, 1.26984116e+00, 0.00000000e+00, ...,\n",
       "        1.36478141e+00, 1.37701242e+00, 1.41421356e+00],\n",
       "       ...,\n",
       "       [1.41421356e+00, 1.35567183e+00, 1.36478141e+00, ...,\n",
       "        0.00000000e+00, 1.37165220e+00, 1.41421356e+00],\n",
       "       [1.41421356e+00, 1.40962530e+00, 1.37701242e+00, ...,\n",
       "        1.37165220e+00, 0.00000000e+00, 1.20904163e+00],\n",
       "       [1.36594777e+00, 1.41421356e+00, 1.41421356e+00, ...,\n",
       "        1.41421356e+00, 1.20904163e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_mx = euclidean_distances(pos_mx, pos_mx)\n",
    "cs_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review: \n",
      " Greenies have been a part of my dogs lives for nearly thirteen years and they look forward to one everyday.  I ordered them from Amazon and everything went smoothly.  I did not have a hitch in the order or length of time getting them. Thank you so much, Linda Turner  I would recommend ordering these to anyone. \n",
      "------------------------\n",
      " The sencond review: \n",
      " My dog is goofy enough but when he knows one of these are comming,  he really goes nuts.  I did not have a hitch in the order or length of time getting them. Thank you so much, Linda Turner I would recommend ordering these to anyone. Greenies have been a part of my dogs lives for nearly thirteen years and they look forward to one everyday. I ordered them from Amazon and everything went smoothly.\n",
      "The similarity score was 0.4042\n",
      "456 460\n"
     ]
    }
   ],
   "source": [
    "similar_doc_ed(pos_mx, pos_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (3pts)\n",
    "\n",
    "You are an NLP data scientist working at Fandango. You observe the following dataset in your review comments:\n",
    "\n",
    "**Intent to Buy Tickets:**\n",
    "1.\tLove this movie. Can’t wait!\n",
    "2.\tI want to see this movie so bad.\n",
    "3.\tThis movie looks amazing.\n",
    "\n",
    "**No Intent to Buy Tickets:**\n",
    "1.\tLooks bad.\n",
    "2.\tHard pass to see this bad movie.\n",
    "3.\tSo boring!\n",
    "\n",
    "You can consider the following stopwords for removal: `to`, `this`.\n",
    "\n",
    "Is the following review an `Intent to Buy` or `No Intent to Buy`? Show your work for each computation.\n",
    "> This looks so bad.\n",
    "\n",
    "You'll need to compute:\n",
    "* Prior\n",
    "* Likelihood\n",
    "* Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prior:**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(y = Intent) &= 1/2   \\\\\n",
    "P(y = No Intent) &= 1/2 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "**Likelihood:**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(x \\mid y = Intent) & = P(x = \"looks\" \\mid y = Intent) * P(x = \"so\" \\mid y = Intent) * P(x = \"bad\" \\mid y = Intent)   \\\\\n",
    "& = (1/3)/(1/2) * (1/3)/(1/2) * (2/3)/(1/2) \\\\\n",
    "& = 1/6 * 1/6 * 1/3 \\\\\n",
    "& = 1/108 \\\\\n",
    "\\\\\n",
    "\\\\\n",
    "P(x \\mid y = No Intent) & = P(x = \"looks\" \\mid y = No Intent) * P(x = \"so\" \\mid y = No Intent) * P(x = \"bad\" \\mid y = No Intent)   \\\\\n",
    "& = (1/3)/(1/2) * (1/3)/(1/2) * (1/3)/(1/2) \\\\\n",
    "& = 1/6 * 1/6 * 1/6 \\\\\n",
    "& = 1/216 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "**Evidence:**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(x) &= P(x \\mid y = Intent)*P(y = Intent) + P(x \\mid y = No Intent)*P(y = No Intent)   \\\\\n",
    "& = (1/108) * (1/2) + (1/216) * (1/2) \\\\\n",
    "& = 1/144 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "**Posterior:**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(y = Intent \\mid x) &= (P(x \\mid y = Intent) * P(y = Intent))/ P(x)  \\\\\n",
    "& = ((1/108) * (1/2)) / (1/144) \\\\\n",
    "& = 2/3 \\\\\n",
    "\\\\\n",
    "\\\\\n",
    "P(y = No Intent \\mid x) &= (P(x \\mid y = No Intent) * P(y = No Intent))/ P(x) \\\\\n",
    "& = ((1/216) * (1/2)) / (1/144) \\\\\n",
    "& = 1/3 \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the posterior probability,  \"This looks so bad.\" will be classified as Intent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
