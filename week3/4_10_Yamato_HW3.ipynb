{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3\n",
    "\n",
    "Submit via Slack. Due on **Tuesday, April 12th, 2022, 6:29pm PST**. You may work with one other person.\n",
    "## TF-IDF (5pts)\n",
    "\n",
    "You are an analyst working for Amazon's product team, and charged with identifying areas for improvement for the toy reviews.\n",
    "\n",
    "Using the **amazon-fine-foods.csv** dataset, clean and parse the text reviews. Explain the decisions you make:\n",
    "- why remove/keep stopwords?\n",
    "- which stopwords to remove?\n",
    "- stemming versus lemmatization?\n",
    "- regex cleaning and substitution?\n",
    "- adding in custom stopwords?\n",
    "- what `n` for your `n-grams`?\n",
    "\n",
    "Finally, generate a TF-IDF report that explains for a business (non-technical) stakeholder:\n",
    "* the features your analysis showed that customers cited as reasons for a poor review\n",
    "* the features your analysis showed that customers cited as reasons for a good review\n",
    "* the most common issues identified from your analysis that generated customer dissatisfaction.\n",
    "\n",
    "Explain to what degree the TF-IDF findings make sense - what are its limitations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize text into words. Convert texts to lower case.\n",
    "    Remove hashtags, punctuations, stopwords, website links, extra spaces, non-alphanumeric characters and \n",
    "    single character. stemtize texts.\n",
    "    \"\"\"\n",
    "\n",
    "    # erase html language characters\n",
    "    html = re.compile(r'<.*?>')\n",
    "    text = html.sub(r'',text)\n",
    "\n",
    "    # year phrases\n",
    "    text = re.sub(r'(\\-?yrs?)', ' year', text)\n",
    "    text = re.sub(r'(\\-?years?\\-?olds?)', ' year old', text)\n",
    "\n",
    "    # birthday\n",
    "    text = re.sub(r'([Bb]\\-?[Dd]ays?)', 'birthday', text)\n",
    "\n",
    "    # holiday words\n",
    "    text = re.sub(r'([Xx][Mm]as|[Cc]hrist\\-[Mm]as)', 'christmas', text)\n",
    "    text = re.sub(r'([Nn]ew\\-[Yy]ears?)', 'new years', text)\n",
    "\n",
    "    tokens = [token for token in nltk.word_tokenize(text)]\n",
    "    \n",
    "    # Combine stopwords and punctuation\n",
    "    stops = stopwords.words(\"english\") + list(string.punctuation)\n",
    "\n",
    "    # # adding extra stopwords (buy, bought, purchase, purchased)\n",
    "    stops.append('buy')\n",
    "    stops.append('bought')\n",
    "    stops.append('purchase')\n",
    "    stops.append('purchased')\n",
    "\n",
    "    ## the following codes are from my past nlp project that I use when cleaning the text/ tokens\n",
    "\n",
    "    # special characters\n",
    "    s_chars = '¥₽ÏïŰŬĎŸæ₿₪ÚŇÀèÅ”ĜåŽÖéříÿý€ŝĤ₹áŜŮÂ₴ûÌÇšŘúüëÓ₫ŠčÎŤÆÒœ₩öËäøÍťìĈôàĥÝ¢ç“žðÙÊĉŭÈŒÐÉÔĵùÁů„âÄűĴóêĝÞîØòď฿ČÜþňÛ'\n",
    "    \n",
    "    # Create PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    tokens_no_hashtag = [re.sub(r'#', '', token) for token in tokens]\n",
    "    tokens_no_stopwords = [token.lower() for token in tokens_no_hashtag if token.lower() not in stops]\n",
    "    tokens_no_url = [re.sub(r'http\\S+', '', token) for token in tokens_no_stopwords]\n",
    "    tokens_no_url = [re.sub(r'www\\S+', '', token) for token in tokens_no_url]\n",
    "    tokens_no_special_char = [re.sub(r'[{}]'.format(s_chars), '', token) for token in tokens_no_url]\n",
    "    tokens_no_extra_space = [re.sub(r'\\s\\s+', '', token) for token in tokens_no_special_char]\n",
    "    tokens_alnum = [token for token in tokens_no_extra_space if token.isalnum()]\n",
    "    tokens_stem = [stemmer.stem(token) for token in tokens_alnum]\n",
    "    tokens_final = [token for token in tokens_stem if len(token) > 1]\n",
    "    \n",
    "    return ' '.join(tokens_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which stopwords to remove? Why remove/keep stopwords? Adding in custom stopwords?\n",
    "    - which words we removed\n",
    "    - which words we kept\n",
    "    - which words we added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stemming versus lemmatization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- regex cleaning and substitution?\n",
    "    - cleaned html format strings\n",
    "    - cleaned phrases like `year`, `birthday`, and holiday words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- what `n` for your `n-grams`?\n",
    "    - we chose n = 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20983</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A21U4DR8M6I9QN</td>\n",
       "      <td>K. M Merrill \"justine\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1318896000</td>\n",
       "      <td>addictive! but works for night coughing in dogs</td>\n",
       "      <td>my 12 year old sheltie has chronic brochotitis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20984</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A17TDUBB4Z1PEC</td>\n",
       "      <td>jaded_green</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1318550400</td>\n",
       "      <td>genuine Greenies best price</td>\n",
       "      <td>These are genuine Greenies product, not a knoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20985</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>ABQH3WAWMSMBH</td>\n",
       "      <td>tenisbrat87</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1317168000</td>\n",
       "      <td>Perfect for our little doggies</td>\n",
       "      <td>Our dogs love Greenies, but of course, which d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   ProductId          UserId             ProfileName  \\\n",
       "0  20983  B002QWP89S  A21U4DR8M6I9QN  K. M Merrill \"justine\"   \n",
       "1  20984  B002QWP89S  A17TDUBB4Z1PEC             jaded_green   \n",
       "2  20985  B002QWP89S   ABQH3WAWMSMBH             tenisbrat87   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1318896000   \n",
       "1                     1                       1      5  1318550400   \n",
       "2                     1                       1      5  1317168000   \n",
       "\n",
       "                                           Summary  \\\n",
       "0  addictive! but works for night coughing in dogs   \n",
       "1                      genuine Greenies best price   \n",
       "2                   Perfect for our little doggies   \n",
       "\n",
       "                                                Text  \n",
       "0  my 12 year old sheltie has chronic brochotitis...  \n",
       "1  These are genuine Greenies product, not a knoc...  \n",
       "2  Our dogs love Greenies, but of course, which d...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('amazon_fine_foods.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B007JFMH8M    913\n",
       "B0026RQTGE    632\n",
       "B002QWP8H0    632\n",
       "B002QWHJOU    632\n",
       "B002QWP89S    632\n",
       "B003B3OOPA    623\n",
       "B001EO5Q64    567\n",
       "B000VK8AVK    564\n",
       "B007M83302    564\n",
       "B001RVFEP2    564\n",
       "Name: ProductId, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ProductId.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2168, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## limiting our data to just 3 products\n",
    "df = df[df.ProductId.isin(['B007JFMH8M', 'B0026RQTGE', 'B003B3OOPA'])]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the word counts from summary and review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary_tok'] = df.Summary.apply(clean_text)\n",
    "df['text_tok'] = df.Text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(str):\n",
    "    counts = dict()\n",
    "    words = str.split()\n",
    "\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cooki</th>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>greeni</th>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soft</th>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yummi</th>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oil</th>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coconut</th>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delici</th>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product</th>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quaker</th>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bake</th>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oatmeal</th>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treat</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tast</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yum</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words\n",
       "love       307\n",
       "great      307\n",
       "cooki      245\n",
       "dog        208\n",
       "good       199\n",
       "greeni     180\n",
       "soft       174\n",
       "yummi      158\n",
       "oil        128\n",
       "coconut    119\n",
       "delici     116\n",
       "product    112\n",
       "quaker     108\n",
       "bake        95\n",
       "oatmeal     84\n",
       "treat       67\n",
       "tast        67\n",
       "best        58\n",
       "price       57\n",
       "yum         49"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary\n",
    "sum_l = df['summary_tok'].astype(str).values.tolist()\n",
    "summary_text = ' '.join(sum_l)\n",
    "summary_df = pd.DataFrame.from_dict(word_count(summary_text), orient='index',columns=['words'])\n",
    "summary_df.sort_values(by = 'words', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cooki</th>\n",
       "      <td>1677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>1247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oil</th>\n",
       "      <td>944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coconut</th>\n",
       "      <td>803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product</th>\n",
       "      <td>798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soft</th>\n",
       "      <td>787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tast</th>\n",
       "      <td>649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>greeni</th>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hair</th>\n",
       "      <td>565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tri</th>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oatmeal</th>\n",
       "      <td>494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realli</th>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treat</th>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skin</th>\n",
       "      <td>382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raisin</th>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also</th>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quaker</th>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eat</th>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make</th>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teeth</th>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go</th>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>box</th>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bake</th>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>even</th>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>littl</th>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>give</th>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smell</th>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>much</th>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well</th>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words\n",
       "cooki     1677\n",
       "love      1247\n",
       "use        982\n",
       "oil        944\n",
       "dog        837\n",
       "coconut    803\n",
       "product    798\n",
       "soft       787\n",
       "great      766\n",
       "like       749\n",
       "good       720\n",
       "one        660\n",
       "tast       649\n",
       "greeni     644\n",
       "hair       565\n",
       "tri        557\n",
       "get        508\n",
       "oatmeal    494\n",
       "would      409\n",
       "realli     388\n",
       "treat      386\n",
       "skin       382\n",
       "raisin     376\n",
       "also       356\n",
       "quaker     341\n",
       "eat        333\n",
       "make       331\n",
       "teeth      318\n",
       "go         315\n",
       "box        308\n",
       "bake       295\n",
       "time       295\n",
       "even       292\n",
       "price      291\n",
       "littl      281\n",
       "day        279\n",
       "give       275\n",
       "smell      271\n",
       "much       266\n",
       "well       260"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review\n",
    "review_l = df['text_tok'].astype(str).values.tolist()\n",
    "review_text = ' '.join(review_l)\n",
    "review_df = pd.DataFrame.from_dict(word_count(review_text), orient='index',columns=['words'])\n",
    "review_df.sort_values(by = 'words', ascending=False).head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newly added stopwords\n",
    "* product/ products\n",
    "* amazon\n",
    "\n",
    "* package\n",
    "* want, made, would, could, give, said, say\n",
    "* realli\n",
    "* lol\n",
    "\n",
    "### regex substitution\n",
    "* like/ good/ great/ love/ nice as POSITIVE\n",
    "* bake/ cookie/ oatmeal/ quaker as FOOD_PRODUCT_1\n",
    "* dog/ greenie/ treat as FOOD_PRODUCT_2\n",
    "* coconut oil/ as FOOD_PRODUCT_3\n",
    "* numerical number (170, 20,...) as NUMBER\n",
    "* tasty/ delicious/ flavor as ...\n",
    "* repeating letter words (mmmmmmmm, aaaa. ect)  --> when it repeats more than 3 times, omit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## divide the reviews into positive, netual, and negative\n",
    "def pos_neg(x):\n",
    "    if x  >= 4:\n",
    "        return 'P'\n",
    "    else:\n",
    "        return 'Ng'\n",
    "\n",
    "df['sensetive'] = df.Score.apply(lambda x: pos_neg(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataframe into two parts\n",
    "pos_df = df[df.sensetive == 'P']\n",
    "neg_df = df[df.sensetive == 'Ng']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamato0615/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/yamato0615/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "pos_df['text_tok'] = pos_df.Text.apply(clean_text)\n",
    "neg_df['text_tok'] = neg_df.Text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(doc):\n",
    "    # min_df = 0.01, since we wanted to reduce dimensionality and take away words that were not commonly used\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df = 0.01)\n",
    "    X = vectorizer.fit_transform(doc) \n",
    "    X = X.toarray()\n",
    "    return pd.DataFrame(X, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamato0615/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "neg_mx = to_matrix(neg_df['text_tok'])\n",
    "pos_mx = to_matrix(pos_df['text_tok'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100</th>\n",
       "      <th>12</th>\n",
       "      <th>12g</th>\n",
       "      <th>12g sugar</th>\n",
       "      <th>13</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>170</th>\n",
       "      <th>170 calori</th>\n",
       "      <th>20</th>\n",
       "      <th>...</th>\n",
       "      <th>write</th>\n",
       "      <th>wrong</th>\n",
       "      <th>ye</th>\n",
       "      <th>year</th>\n",
       "      <th>year old</th>\n",
       "      <th>yearup</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>yorki</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.164452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.110238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.165011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.117535</td>\n",
       "      <td>0.122193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 1276 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     100   12  12g  12g sugar   13   16   17       170  170 calori   20  ...  \\\n",
       "0    0.0  0.0  0.0        0.0  0.0  0.0  0.0  0.000000    0.000000  0.0  ...   \n",
       "1    0.0  0.0  0.0        0.0  0.0  0.0  0.0  0.000000    0.000000  0.0  ...   \n",
       "2    0.0  0.0  0.0        0.0  0.0  0.0  0.0  0.000000    0.000000  0.0  ...   \n",
       "3    0.0  0.0  0.0        0.0  0.0  0.0  0.0  0.000000    0.000000  0.0  ...   \n",
       "4    0.0  0.0  0.0        0.0  0.0  0.0  0.0  0.000000    0.000000  0.0  ...   \n",
       "..   ...  ...  ...        ...  ...  ...  ...       ...         ...  ...  ...   \n",
       "165  0.0  0.0  0.0        0.0  0.0  0.0  0.0  0.110238    0.000000  0.0  ...   \n",
       "166  0.0  0.0  0.0        0.0  0.0  0.0  0.0  0.000000    0.000000  0.0  ...   \n",
       "167  0.0  0.0  0.0        0.0  0.0  0.0  0.0  0.000000    0.000000  0.0  ...   \n",
       "168  0.0  0.0  0.0        0.0  0.0  0.0  0.0  0.000000    0.000000  0.0  ...   \n",
       "169  0.0  0.0  0.0        0.0  0.0  0.0  0.0  0.117535    0.122193  0.0  ...   \n",
       "\n",
       "     write     wrong   ye      year  year old  yearup  yesterday  yet  yorki  \\\n",
       "0      0.0  0.000000  0.0  0.000000       0.0     0.0        0.0  0.0    0.0   \n",
       "1      0.0  0.000000  0.0  0.060841       0.0     0.0        0.0  0.0    0.0   \n",
       "2      0.0  0.164452  0.0  0.000000       0.0     0.0        0.0  0.0    0.0   \n",
       "3      0.0  0.000000  0.0  0.000000       0.0     0.0        0.0  0.0    0.0   \n",
       "4      0.0  0.000000  0.0  0.000000       0.0     0.0        0.0  0.0    0.0   \n",
       "..     ...       ...  ...       ...       ...     ...        ...  ...    ...   \n",
       "165    0.0  0.000000  0.0  0.000000       0.0     0.0        0.0  0.0    0.0   \n",
       "166    0.0  0.000000  0.0  0.000000       0.0     0.0        0.0  0.0    0.0   \n",
       "167    0.0  0.000000  0.0  0.000000       0.0     0.0        0.0  0.0    0.0   \n",
       "168    0.0  0.000000  0.0  0.000000       0.0     0.0        0.0  0.0    0.0   \n",
       "169    0.0  0.000000  0.0  0.000000       0.0     0.0        0.0  0.0    0.0   \n",
       "\n",
       "        yummi  \n",
       "0    0.000000  \n",
       "1    0.000000  \n",
       "2    0.000000  \n",
       "3    0.000000  \n",
       "4    0.000000  \n",
       "..        ...  \n",
       "165  0.165011  \n",
       "166  0.000000  \n",
       "167  0.000000  \n",
       "168  0.000000  \n",
       "169  0.000000  \n",
       "\n",
       "[170 rows x 1276 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the TF-idf score for each tokens (n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_neg = neg_mx.T\n",
    "tf_neg['score'] = tf_neg.sum(axis =1)\n",
    "tf_pos = pos_mx.T\n",
    "tf_pos['score'] = tf_pos.sum(axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### top 20 scores for positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cooki</th>\n",
       "      <td>145.951363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>114.116687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>92.750532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>89.465790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>86.995125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oil</th>\n",
       "      <td>83.022097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soft</th>\n",
       "      <td>82.763034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product</th>\n",
       "      <td>76.951207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>76.478984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>greeni</th>\n",
       "      <td>75.995298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coconut</th>\n",
       "      <td>73.515477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>70.985732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tast</th>\n",
       "      <td>67.999348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>66.755465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hair</th>\n",
       "      <td>60.088874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coconut oil</th>\n",
       "      <td>59.248181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tri</th>\n",
       "      <td>57.821059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>56.661260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oatmeal</th>\n",
       "      <td>55.459008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treat</th>\n",
       "      <td>51.082020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  score\n",
       "cooki        145.951363\n",
       "love         114.116687\n",
       "dog           92.750532\n",
       "great         89.465790\n",
       "use           86.995125\n",
       "oil           83.022097\n",
       "soft          82.763034\n",
       "product       76.951207\n",
       "good          76.478984\n",
       "greeni        75.995298\n",
       "coconut       73.515477\n",
       "like          70.985732\n",
       "tast          67.999348\n",
       "one           66.755465\n",
       "hair          60.088874\n",
       "coconut oil   59.248181\n",
       "tri           57.821059\n",
       "get           56.661260\n",
       "oatmeal       55.459008\n",
       "treat         51.082020"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_pos_score = tf_pos[['score']].sort_values(by = 'score',ascending = False).head(20)\n",
    "tf_pos_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### top 20 scores for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cooki</th>\n",
       "      <td>12.842220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>8.881612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>greeni</th>\n",
       "      <td>7.451771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>6.200713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product</th>\n",
       "      <td>6.165560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tast</th>\n",
       "      <td>5.675585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>5.413039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>5.080215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>5.034909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>4.909922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soft</th>\n",
       "      <td>4.864850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>4.853921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oatmeal</th>\n",
       "      <td>4.523758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tri</th>\n",
       "      <td>4.521451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oil</th>\n",
       "      <td>4.027458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coconut</th>\n",
       "      <td>3.802558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raisin</th>\n",
       "      <td>3.793862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hair</th>\n",
       "      <td>3.672902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chew</th>\n",
       "      <td>3.646216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>littl</th>\n",
       "      <td>3.604645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             score\n",
       "cooki    12.842220\n",
       "dog       8.881612\n",
       "greeni    7.451771\n",
       "like      6.200713\n",
       "product   6.165560\n",
       "tast      5.675585\n",
       "use       5.413039\n",
       "would     5.080215\n",
       "one       5.034909\n",
       "love      4.909922\n",
       "soft      4.864850\n",
       "good      4.853921\n",
       "oatmeal   4.523758\n",
       "tri       4.521451\n",
       "oil       4.027458\n",
       "coconut   3.802558\n",
       "raisin    3.793862\n",
       "hair      3.672902\n",
       "chew      3.646216\n",
       "littl     3.604645"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_neg_score = tf_neg[['score']].sort_values(by = 'score',ascending = False).head(20)\n",
    "tf_neg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF report that explains for a business (non-technical) stakeholder:\n",
    "* the features your analysis showed that customers cited as reasons for a poor review\n",
    "* the features your analysis showed that customers cited as reasons for a good review\n",
    "* the most common issues identified from your analysis that generated customer dissatisfaction.\n",
    "\n",
    "Explain to what degree the TF-IDF findings make sense - what are its limitations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The features your analysis showed that customers cited as reasons for a poor review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The features your analysis showed that customers cited as reasons for a good review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The most common issues identified from your analysis that generated customer dissatisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity and Word Embeddings (2 pts)\n",
    "\n",
    "Using\n",
    "* `TfIdfVectorizer`\n",
    "\n",
    "Identify the most similar pair of reviews from the `amazon-fine-foods.csv` dataset using both Euclidean distance and cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cosine similarity function\n",
    "def similar_doc_cs(vec_df, ori_df):\n",
    "    \"\"\"\n",
    "    Inputs two arguments: vectorized dataframe and the original dataframe\n",
    "    Ouput is the two reviews that had the highest similarity and the score\n",
    "\n",
    "    This function uses the cosine similarity and retrieves the highest value besides 1\n",
    "    \"\"\"\n",
    "    cs_mx = cosine_similarity(vec_df, vec_df)\n",
    "    mod = np.where(cs_mx >= .99, 0, cs_mx)\n",
    "    indices = np.where(mod == mod.max())\n",
    "\n",
    "    count = 0\n",
    "    for n,i in enumerate(mod):\n",
    "        for m,j in enumerate(i):\n",
    "            if j >= mod[indices][0] - 0.001 and n < m and count < 1:\n",
    "                count += 1\n",
    "                print(f'The first review:', '\\n', ori_df.iloc[m,9], '\\n------------------------\\n', \n",
    "            f'The sencond review:', '\\n', ori_df.iloc[n,9])\n",
    "                print(f'The similarity score was {round(j,4)}')\n",
    "\n",
    "\n",
    "## euclidean distance function\n",
    "def similar_doc_ed(vec_df, ori_df):\n",
    "    \"\"\"\n",
    "    Inputs two arguments: vectorized dataframe and the original dataframe\n",
    "    Ouput is the two reviews that had the highest similarity and the score\n",
    "\n",
    "    This function uses the euclidean distance and retrieves the highest value besides 1\n",
    "    \"\"\"\n",
    "    cs_mx = euclidean_distances(vec_df, vec_df)\n",
    "    mod = np.where(cs_mx <= .009, 100, cs_mx)\n",
    "    indices = np.where(mod == mod.min())\n",
    "\n",
    "    count = 0\n",
    "    for n,i in enumerate(mod):\n",
    "        for m,j in enumerate(i):\n",
    "            if j <= mod[indices][0] and n < m and count < 1:\n",
    "                count += 1\n",
    "                print(f'The first review:', '\\n', ori_df.iloc[m,9], '\\n------------------------\\n', \n",
    "            f'The sencond review:', '\\n', ori_df.iloc[n,9])\n",
    "                print(f'The similarity score was {round(j,4)}')\n",
    "                print(n,m)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a dog trainer and have never seen  anything like it....three weeks later,, the beloved sheltie got a bowel blockage from these, use with caution.if the cat gets too many she has the runs....sheltie did better when i upped her thryoid meds, and gave her doggie asthma meds.s\n"
     ]
    }
   ],
   "source": [
    "t = \"I am a dog trainer and have never seen  anything like it....<br /><br />three weeks later,, the beloved sheltie got a bowel blockage from these, use with caution.<br />if the cat gets too many she has the runs....<br />sheltie did better when i upped her thryoid meds, and gave her doggie asthma meds.<br />s\"\n",
    "html = re.compile(r'<.*?>')\n",
    "text = html.sub(r'',t)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review: \n",
      " I purchased this due to my use of coconut oil products in my hair. I am currently using the Coconut Oil by Organic Root Stimulator and decided to go with something more natural since ORS contains petrolatum. While I'm sure this product works great I cannot attest to that because the smell was nauseating to me. If you love the strong smell of coconut then this may be for you. \n",
      "------------------------\n",
      " The sencond review: \n",
      " I buy and use coconut oil often.  I use it to cook with, spread on various foods, take it for my health, use on my skin and hair; kind of a coconut oil junkie.  I bought this to try thinking it was better bang for my buck.  It works well on my skin and hair but in my humble opinion the taste is terrible.  I much prefer the taste of Coconut Pacific 100% Pure Organic Extra Virgin Raw Coconut Oil.  I hope Coconut Pacific's <a href=\"http://www.amazon.com/gp/product/B0011DHM8S\">Coconut Oil Extra Virgin Organic 14 Oz - Noni Pacific</a> will come back into stock soon as I mainly use Coconut Oil and Olive Oil for day to day cooking.<br /><br />So, if you are seeking coconut oil for your health where you eat up to a tablespoon a day - do not buy this as you will never get it down.<br /><br />If you are only looking to use on your hair and skin this should be okay.\n",
      "The similarity score was 0.5111\n"
     ]
    }
   ],
   "source": [
    "similar_doc_cs(neg_mx, neg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review: \n",
      " I purchased this due to my use of coconut oil products in my hair. I am currently using the Coconut Oil by Organic Root Stimulator and decided to go with something more natural since ORS contains petrolatum. While I'm sure this product works great I cannot attest to that because the smell was nauseating to me. If you love the strong smell of coconut then this may be for you. \n",
      "------------------------\n",
      " The sencond review: \n",
      " I buy and use coconut oil often.  I use it to cook with, spread on various foods, take it for my health, use on my skin and hair; kind of a coconut oil junkie.  I bought this to try thinking it was better bang for my buck.  It works well on my skin and hair but in my humble opinion the taste is terrible.  I much prefer the taste of Coconut Pacific 100% Pure Organic Extra Virgin Raw Coconut Oil.  I hope Coconut Pacific's <a href=\"http://www.amazon.com/gp/product/B0011DHM8S\">Coconut Oil Extra Virgin Organic 14 Oz - Noni Pacific</a> will come back into stock soon as I mainly use Coconut Oil and Olive Oil for day to day cooking.<br /><br />So, if you are seeking coconut oil for your health where you eat up to a tablespoon a day - do not buy this as you will never get it down.<br /><br />If you are only looking to use on your hair and skin this should be okay.\n",
      "The similarity score was 0.9889\n",
      "67 79\n"
     ]
    }
   ],
   "source": [
    "similar_doc_ed(neg_mx, neg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review: \n",
      " Greenies have been a part of my dogs lives for nearly thirteen years and they look forward to one everyday.  I ordered them from Amazon and everything went smoothly.  I did not have a hitch in the order or length of time getting them. Thank you so much, Linda Turner  I would recommend ordering these to anyone. \n",
      "------------------------\n",
      " The sencond review: \n",
      " My dog is goofy enough but when he knows one of these are comming,  he really goes nuts.  I did not have a hitch in the order or length of time getting them. Thank you so much, Linda Turner I would recommend ordering these to anyone. Greenies have been a part of my dogs lives for nearly thirteen years and they look forward to one everyday. I ordered them from Amazon and everything went smoothly.\n",
      "The similarity score was 0.9353\n"
     ]
    }
   ],
   "source": [
    "similar_doc_cs(pos_mx, pos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 1.37833303e+00, 1.33597703e+00, ...,\n",
       "        1.41421356e+00, 1.41421356e+00, 1.36611238e+00],\n",
       "       [1.37833303e+00, 2.10734243e-08, 1.23037517e+00, ...,\n",
       "        1.34566340e+00, 1.40838928e+00, 1.41421356e+00],\n",
       "       [1.33597703e+00, 1.23037517e+00, 0.00000000e+00, ...,\n",
       "        1.37091210e+00, 1.38063291e+00, 1.41421356e+00],\n",
       "       ...,\n",
       "       [1.41421356e+00, 1.34566340e+00, 1.37091210e+00, ...,\n",
       "        0.00000000e+00, 1.37096979e+00, 1.41421356e+00],\n",
       "       [1.41421356e+00, 1.40838928e+00, 1.38063291e+00, ...,\n",
       "        1.37096979e+00, 0.00000000e+00, 1.20791871e+00],\n",
       "       [1.36611238e+00, 1.41421356e+00, 1.41421356e+00, ...,\n",
       "        1.41421356e+00, 1.20791871e+00, 2.10734243e-08]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_mx = euclidean_distances(pos_mx, pos_mx)\n",
    "cs_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review: \n",
      " Greenies have been a part of my dogs lives for nearly thirteen years and they look forward to one everyday.  I ordered them from Amazon and everything went smoothly.  I did not have a hitch in the order or length of time getting them. Thank you so much, Linda Turner  I would recommend ordering these to anyone. \n",
      "------------------------\n",
      " The sencond review: \n",
      " My dog is goofy enough but when he knows one of these are comming,  he really goes nuts.  I did not have a hitch in the order or length of time getting them. Thank you so much, Linda Turner I would recommend ordering these to anyone. Greenies have been a part of my dogs lives for nearly thirteen years and they look forward to one everyday. I ordered them from Amazon and everything went smoothly.\n",
      "The similarity score was 0.3596\n",
      "456 460\n"
     ]
    }
   ],
   "source": [
    "similar_doc_ed(pos_mx, pos_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (3pts)\n",
    "\n",
    "You are an NLP data scientist working at Fandango. You observe the following dataset in your review comments:\n",
    "\n",
    "**Intent to Buy Tickets:**\n",
    "1.\tLove this movie. Can’t wait!\n",
    "2.\tI want to see this movie so bad.\n",
    "3.\tThis movie looks amazing.\n",
    "\n",
    "**No Intent to Buy Tickets:**\n",
    "1.\tLooks bad.\n",
    "2.\tHard pass to see this bad movie.\n",
    "3.\tSo boring!\n",
    "\n",
    "You can consider the following stopwords for removal: `to`, `this`.\n",
    "\n",
    "Is the following review an `Intent to Buy` or `No Intent to Buy`? Show your work for each computation.\n",
    "> This looks so bad.\n",
    "\n",
    "You'll need to compute:\n",
    "* Prior\n",
    "* Likelihood\n",
    "* Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prior:**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(y = Intent) &= 1/2   \\\\\n",
    "P(y = No Intent) &= 1/2 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "**Likelihood:**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(x \\mid y = Intent) & = P(x = \"looks\" \\mid y = Intent) * P(x = \"so\" \\mid y = Intent) * P(x = \"bad\" \\mid y = Intent)   \\\\\n",
    "& = (1/3)/(1/2) * (1/3)/(1/2) * (2/3)/(1/2) \\\\\n",
    "& = 1/6 * 1/6 * 1/3 \\\\\n",
    "& = 1/108 \\\\\n",
    "\\\\\n",
    "\\\\\n",
    "P(x \\mid y = No Intent) & = P(x = \"looks\" \\mid y = No Intent) * P(x = \"so\" \\mid y = No Intent) * P(x = \"bad\" \\mid y = No Intent)   \\\\\n",
    "& = (1/3)/(1/2) * (1/3)/(1/2) * (1/3)/(1/2) \\\\\n",
    "& = 1/6 * 1/6 * 1/6 \\\\\n",
    "& = 1/216 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "**Evidence:**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(x) &= P(x \\mid y = Intent)*P(y = Intent) + P(x \\mid y = No Intent)*P(y = No Intent)   \\\\\n",
    "& = (1/108) * (1/2) + (1/216) * (1/2) \\\\\n",
    "& = 1/144 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "**Posterior:**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(y = Intent \\mid x) &= (P(x \\mid y = Intent) * P(y = Intent))/ P(x)  \\\\\n",
    "& = ((1/108) * (1/2)) / (1/144) \\\\\n",
    "& = 2/3 \\\\\n",
    "\\\\\n",
    "\\\\\n",
    "P(y = No Intent \\mid x) &= (P(x \\mid y = No Intent) * P(y = No Intent))/ P(x) \\\\\n",
    "& = ((1/216) * (1/2)) / (1/144) \\\\\n",
    "& = 1/3 \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the posterior probability,  \"This looks so bad.\" will be classified as Intent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
