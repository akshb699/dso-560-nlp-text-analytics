{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3\n",
    "\n",
    "Submit via Slack. Due on **Tuesday, April 12th, 2022, 6:29pm PST**. You may work with one other person.\n",
    "## TF-IDF (5pts)\n",
    "\n",
    "You are an analyst working for Amazon's product team, and charged with identifying areas for improvement for the toy reviews.\n",
    "\n",
    "Using the **amazon-fine-foods.csv** dataset, clean and parse the text reviews. Explain the decisions you make:\n",
    "- why remove/keep stopwords?\n",
    "- which stopwords to remove?\n",
    "- stemming versus lemmatization?\n",
    "- regex cleaning and substitution?\n",
    "- adding in custom stopwords?\n",
    "- what `n` for your `n-grams`?\n",
    "\n",
    "Finally, generate a TF-IDF report that explains for a business (non-technical) stakeholder:\n",
    "* the features your analysis showed that customers cited as reasons for a poor review\n",
    "* the features your analysis showed that customers cited as reasons for a good review\n",
    "* the most common issues identified from your analysis that generated customer dissatisfaction.\n",
    "\n",
    "Explain to what degree the TF-IDF findings make sense - what are its limitations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize text into words. Convert texts to lower case.\n",
    "    Remove hashtags, punctuations, stopwords, website links, extra spaces, non-alphanumeric characters and \n",
    "    single character. stemtize texts.\n",
    "    \"\"\"\n",
    "\n",
    "    # erase html language characters\n",
    "    html = re.compile(r'<.*?>')\n",
    "    text = html.sub(r'',text)\n",
    "\n",
    "    # product replacement\n",
    "    text = re.sub(r'\\b(cookiee?s?)\\b', 'FOOD_PRODUCT_1', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(greenies?)\\b', 'FOOD_PRODUCT_2', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(coconut\\s?oils?|oils?)\\b', 'FOOD_PRODUCT_3', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # numerbers plus units\n",
    "    text = re.sub(r'\\b([1-9]+[\\w]*)\\b', '_NUMERIC_', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # taste words\n",
    "    text = re.sub(r'\\b(tasty|delicious|yum*y|enjoy)\\b', 'GOOD_TASTE', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(nasty|disgusting|rotten|stale)\\b', 'BAD_TASTE', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # words with 3 or more of same letters\n",
    "    text = re.sub(r'\\b[a-z0-9]*(.)\\1\\1+[a-z0-9]*\\b', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    tokens = [token for token in nltk.word_tokenize(text)]\n",
    "    \n",
    "    # Combine stopwords and punctuation\n",
    "    stops = stopwords.words(\"english\") + list(string.punctuation)\n",
    "\n",
    "    # # adding extra stopwords (buy, bought, purchase, purchased)\n",
    "    stops.append('buy')\n",
    "    stops.append('bought')\n",
    "    stops.append('purchase')\n",
    "    stops.append('purchased')\n",
    "    stops.append('product')\n",
    "    stops.append('products')\n",
    "    stops.append('package')\n",
    "    stops.append('packages')\n",
    "\n",
    "    stops.append('quaker')\n",
    "    stops.append('raisin')\n",
    "    stops.append('raisins')\n",
    "    stops.append('bake')\n",
    "    stops.append('baked')\n",
    "    stops.append('oatmeal')\n",
    "    stops.append('dog')\n",
    "    stops.append('dogs')\n",
    "\n",
    "    ## the following codes are from my past nlp project that I use when cleaning the text/ tokens\n",
    "\n",
    "    # special characters\n",
    "    s_chars = '¥₽ÏïŰŬĎŸæ₿₪ÚŇÀèÅ”ĜåŽÖéříÿý€ŝĤ₹áŜŮÂ₴ûÌÇšŘúüëÓ₫ŠčÎŤÆÒœ₩öËäøÍťìĈôàĥÝ¢ç“žðÙÊĉŭÈŒÐÉÔĵùÁů„âÄűĴóêĝÞîØòď฿ČÜþňÛ'\n",
    "    \n",
    "    # Create PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    tokens_no_hashtag = [re.sub(r'#', '', token) for token in tokens]\n",
    "    tokens_no_stopwords = [token.lower() for token in tokens_no_hashtag if token.lower() not in stops]\n",
    "    tokens_no_url = [re.sub(r'http\\S+', '', token) for token in tokens_no_stopwords]\n",
    "    tokens_no_url = [re.sub(r'www\\S+', '', token) for token in tokens_no_url]\n",
    "    tokens_no_special_char = [re.sub(r'[{}]'.format(s_chars), '', token) for token in tokens_no_url]\n",
    "    tokens_no_extra_space = [re.sub(r'\\s\\s+', '', token) for token in tokens_no_special_char]\n",
    "    tokens_alnum = [token for token in tokens_no_extra_space if token.isalnum()]\n",
    "    tokens_stem = [stemmer.stem(token) for token in tokens_alnum]\n",
    "    tokens_final = [token for token in tokens_stem if len(token) > 1]\n",
    "    \n",
    "    return ' '.join(tokens_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "- Stopwords should be removed because we don't want these very common words that are meaningless in our analysis introducing noise and taking up dimensions in our final vectorized matrices.\n",
    "- Stopwords that were removed including all the stopwords in the NLTK corpus, as well as some custom stopwords given the context of this data set. Words like purchase, buy, product, and package were removed because their frequency is likely high due to the data being about food reviews, which makes them relatively meaningless in this context. \n",
    "- Furthermore, since this analysis only focuses on 3 of the top reviewed products from the dataset, and these 3 products all had titles/names that were more than one token long, all but one of those words in the product's name were added to the stopword list (so that only 1 instance of the product being identified in a review could be regex-substituted into a keyword). \n",
    "    - For the Quaker Soft Baked Oatmeal Cookies with Raisin, all relevant naming words other than \"cookie\" were added to the stopword list.\n",
    "    - For the Greenies dog treats, all words except \"Greenies\" were added to the stopword list.\n",
    "    - For the coconut oil, no words were added to the stopword list, and \"coconut oil\" and \"oil\" were considered in the regex substitution section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming vs Lemmatization\n",
    "Since our analysis is focused on counts of words to ultimately do TF-IDF, the part of speech or actual language word doesn't have to be considered. Therefore, stemming was chosen over lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Cleaning and Substitution\n",
    "- Regex substitution was used to change the product name of the products used in this analysis to a keyword specifying which product they were so that we can understand which products were being referenced in a certain review or in our vectorized results.\n",
    "    - \"Cookie\" was used as the identifier for the product with ID \"B007JFMH8M\"\n",
    "    - \"Greenies\" was used as the identifier for the product with ID \"B0026RQTGE\"\n",
    "    - \"Coconut Oil/Oil was used as the identifier for the product with ID \"B003B3OOPA\"\n",
    "- Regex substituion was also used to substitute and clean common words that may appear in a corpus of food reviews, since all of the products we focused on are consumed. These included the words \"tasty\", \"delicious\", \"yummy\", and \"enjoy\" for positive sentiments, and \"nasty\", \"disgusting\", \"rotten\", and \"stale\" for negative ones.\n",
    "- Additionally, regex substitution was used to clean the text:\n",
    "    - Numbers the appear in the corpus were substituted with NUMERIC, since specific numbers don't provide much meaning in an analysis resulting in a vectorized matrix.\n",
    "    - Tokens that contain the same letter three or more times in a row were also removed, since the English language (in almost all cases) outlaws the use of words with 3 or more of the same character in a row, so these erroenous, meaningless words would just add unneeded dimensionality in our final matrices.\n",
    "    - Lastly, regex substituion was used to remove html formatting characters, hastags, URLS, special characters, and extra spaces in the corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram Selection\n",
    "\n",
    "For our n-gram selection, for this analysis we chose 2 and 3. We chose n-gram numbers higher than 1 because since we want to perform TF-IDF on tokens, it’s important to have as much context as possible to understand customer sentiment about positive or negative reviews. More specifically, we found that an n-gram of size 2 works best for getting insight for positive reviews, while an n-gram of size 3 works better for negative reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20983</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A21U4DR8M6I9QN</td>\n",
       "      <td>K. M Merrill \"justine\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1318896000</td>\n",
       "      <td>addictive! but works for night coughing in dogs</td>\n",
       "      <td>my 12 year old sheltie has chronic brochotitis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20984</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A17TDUBB4Z1PEC</td>\n",
       "      <td>jaded_green</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1318550400</td>\n",
       "      <td>genuine Greenies best price</td>\n",
       "      <td>These are genuine Greenies product, not a knoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20985</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>ABQH3WAWMSMBH</td>\n",
       "      <td>tenisbrat87</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1317168000</td>\n",
       "      <td>Perfect for our little doggies</td>\n",
       "      <td>Our dogs love Greenies, but of course, which d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   ProductId          UserId             ProfileName  \\\n",
       "0  20983  B002QWP89S  A21U4DR8M6I9QN  K. M Merrill \"justine\"   \n",
       "1  20984  B002QWP89S  A17TDUBB4Z1PEC             jaded_green   \n",
       "2  20985  B002QWP89S   ABQH3WAWMSMBH             tenisbrat87   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1318896000   \n",
       "1                     1                       1      5  1318550400   \n",
       "2                     1                       1      5  1317168000   \n",
       "\n",
       "                                           Summary  \\\n",
       "0  addictive! but works for night coughing in dogs   \n",
       "1                      genuine Greenies best price   \n",
       "2                   Perfect for our little doggies   \n",
       "\n",
       "                                                Text  \n",
       "0  my 12 year old sheltie has chronic brochotitis...  \n",
       "1  These are genuine Greenies product, not a knoc...  \n",
       "2  Our dogs love Greenies, but of course, which d...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/amazon_fine_foods.csv')\n",
    "# df = pd.read_csv('amazon_fine_foods.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B007JFMH8M    913\n",
       "B0026RQTGE    632\n",
       "B002QWP8H0    632\n",
       "B002QWHJOU    632\n",
       "B002QWP89S    632\n",
       "B003B3OOPA    623\n",
       "B001EO5Q64    567\n",
       "B000VK8AVK    564\n",
       "B007M83302    564\n",
       "B001RVFEP2    564\n",
       "Name: ProductId, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ProductId.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2168, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## limiting our data to just 3 products\n",
    "df = df[df.ProductId.isin(['B007JFMH8M', 'B0026RQTGE', 'B003B3OOPA'])]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the word counts from summary and review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary_tok'] = df.Summary.apply(clean_text)\n",
    "df['text_tok'] = df.Text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(str):\n",
    "    counts = dict()\n",
    "    words = str.split()\n",
    "\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soft</th>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treat</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tast</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yum</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stuff</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hair</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amaz</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chewi</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awesom</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excel</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chew</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wonder</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>healthi</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words\n",
       "love       307\n",
       "great      306\n",
       "good       199\n",
       "soft       174\n",
       "treat       67\n",
       "tast        67\n",
       "best        58\n",
       "price       57\n",
       "yum         48\n",
       "like        45\n",
       "stuff       42\n",
       "hair        41\n",
       "amaz        41\n",
       "chewi       33\n",
       "awesom      33\n",
       "excel       32\n",
       "chew        31\n",
       "use         29\n",
       "wonder      29\n",
       "healthi     29"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary\n",
    "sum_l = df['summary_tok'].astype(str).values.tolist()\n",
    "summary_text = ' '.join(sum_l)\n",
    "summary_df = pd.DataFrame.from_dict(word_count(summary_text), orient='index',columns=['words'])\n",
    "summary_df.sort_values(by = 'words', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>1247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soft</th>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tast</th>\n",
       "      <td>646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hair</th>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tri</th>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realli</th>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treat</th>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skin</th>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also</th>\n",
       "      <td>355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eat</th>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make</th>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teeth</th>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go</th>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>box</th>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>even</th>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>littl</th>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>give</th>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smell</th>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>much</th>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flavor</th>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well</th>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>receiv</th>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cook</th>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>influenst</th>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recommend</th>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kid</th>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snack</th>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazon</th>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>better</th>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           words\n",
       "love        1247\n",
       "use          981\n",
       "soft         785\n",
       "great        764\n",
       "like         749\n",
       "good         720\n",
       "one          658\n",
       "tast         646\n",
       "hair         562\n",
       "tri          557\n",
       "get          508\n",
       "would        408\n",
       "realli       387\n",
       "treat        386\n",
       "skin         379\n",
       "also         355\n",
       "eat          333\n",
       "make         331\n",
       "teeth        317\n",
       "go           313\n",
       "box          309\n",
       "time         293\n",
       "even         291\n",
       "price        288\n",
       "littl        280\n",
       "day          277\n",
       "give         275\n",
       "smell        269\n",
       "much         266\n",
       "flavor       260\n",
       "well         256\n",
       "receiv       247\n",
       "cook         247\n",
       "influenst    245\n",
       "recommend    226\n",
       "kid          224\n",
       "snack        222\n",
       "year         216\n",
       "amazon       202\n",
       "better       201"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review\n",
    "review_l = df['text_tok'].astype(str).values.tolist()\n",
    "review_text = ' '.join(review_l)\n",
    "review_df = pd.DataFrame.from_dict(word_count(review_text), orient='index',columns=['words'])\n",
    "review_df.sort_values(by = 'words', ascending=False).head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## divide the reviews into positive, netual, and negative\n",
    "def pos_neg(x):\n",
    "    if x  >= 4:\n",
    "        return 'P'\n",
    "    else:\n",
    "        return 'Ng'\n",
    "\n",
    "df['sensetive'] = df.Score.apply(lambda x: pos_neg(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataframe into two parts\n",
    "pos_df = df[df.sensetive == 'P']\n",
    "neg_df = df[df.sensetive == 'Ng']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshaybhide/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/akshaybhide/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "pos_df['text_tok'] = pos_df.Text.apply(clean_text)\n",
    "neg_df['text_tok'] = neg_df.Text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(doc,n,m):\n",
    "    # min_df = 0.01, since we wanted to reduce dimensionality and take away words that were not commonly used\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(n, m), min_df = 0.01)\n",
    "    X = vectorizer.fit_transform(doc) \n",
    "    X = X.toarray()\n",
    "    return pd.DataFrame(X, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_mx = to_matrix(neg_df['text_tok'],3,3)\n",
    "pos_mx = to_matrix(pos_df['text_tok'],2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual caus googl</th>\n",
       "      <th>affect stop give</th>\n",
       "      <th>amazon vine program</th>\n",
       "      <th>away bewar bad</th>\n",
       "      <th>bad watch give</th>\n",
       "      <th>becam green sure</th>\n",
       "      <th>bewar bad watch</th>\n",
       "      <th>big glass milk</th>\n",
       "      <th>bland diet boil</th>\n",
       "      <th>boil chicken rice</th>\n",
       "      <th>...</th>\n",
       "      <th>solid day eat</th>\n",
       "      <th>stop give thrown</th>\n",
       "      <th>sure actual caus</th>\n",
       "      <th>thrown away bewar</th>\n",
       "      <th>use skin hair</th>\n",
       "      <th>usual solid day</th>\n",
       "      <th>variou forum affect</th>\n",
       "      <th>whole grain oat</th>\n",
       "      <th>would give star</th>\n",
       "      <th>would take minut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     actual caus googl  affect stop give  amazon vine program  away bewar bad  \\\n",
       "0                  0.0               0.0                  0.0             0.0   \n",
       "1                  0.0               0.0                  0.0             0.0   \n",
       "2                  0.0               0.0                  0.0             0.0   \n",
       "3                  0.0               0.0                  0.0             0.0   \n",
       "4                  0.0               0.0                  0.0             0.0   \n",
       "..                 ...               ...                  ...             ...   \n",
       "165                0.0               0.0                  0.0             0.0   \n",
       "166                0.0               0.0                  0.0             0.0   \n",
       "167                0.0               0.0                  0.0             0.0   \n",
       "168                0.0               0.0                  0.0             0.0   \n",
       "169                0.0               0.0                  0.0             0.0   \n",
       "\n",
       "     bad watch give  becam green sure  bewar bad watch  big glass milk  \\\n",
       "0               0.0               0.0              0.0             0.0   \n",
       "1               0.0               0.0              0.0             0.0   \n",
       "2               0.0               0.0              0.0             0.0   \n",
       "3               0.0               0.0              0.0             0.0   \n",
       "4               0.0               0.0              0.0             0.0   \n",
       "..              ...               ...              ...             ...   \n",
       "165             0.0               0.0              0.0             0.0   \n",
       "166             0.0               0.0              0.0             0.0   \n",
       "167             0.0               0.0              0.0             0.0   \n",
       "168             0.0               0.0              0.0             0.0   \n",
       "169             0.0               0.0              0.0             0.0   \n",
       "\n",
       "     bland diet boil  boil chicken rice  ...  solid day eat  stop give thrown  \\\n",
       "0                0.0                0.0  ...            0.0               0.0   \n",
       "1                0.0                0.0  ...            0.0               0.0   \n",
       "2                0.0                0.0  ...            0.0               0.0   \n",
       "3                0.0                0.0  ...            0.0               0.0   \n",
       "4                0.0                0.0  ...            0.0               0.0   \n",
       "..               ...                ...  ...            ...               ...   \n",
       "165              0.0                0.0  ...            0.0               0.0   \n",
       "166              0.0                0.0  ...            0.0               0.0   \n",
       "167              0.0                0.0  ...            0.0               0.0   \n",
       "168              0.0                0.0  ...            0.0               0.0   \n",
       "169              0.0                0.0  ...            0.0               0.0   \n",
       "\n",
       "     sure actual caus  thrown away bewar  use skin hair  usual solid day  \\\n",
       "0                 0.0                0.0            0.0              0.0   \n",
       "1                 0.0                0.0            0.0              0.0   \n",
       "2                 0.0                0.0            0.0              0.0   \n",
       "3                 0.0                0.0            0.0              0.0   \n",
       "4                 0.0                0.0            0.0              0.0   \n",
       "..                ...                ...            ...              ...   \n",
       "165               0.0                0.0            0.0              0.0   \n",
       "166               0.0                0.0            0.0              0.0   \n",
       "167               0.0                0.0            0.0              0.0   \n",
       "168               0.0                0.0            0.0              0.0   \n",
       "169               0.0                0.0            0.0              0.0   \n",
       "\n",
       "     variou forum affect  whole grain oat  would give star  would take minut  \n",
       "0                    0.0              0.0              0.0               1.0  \n",
       "1                    0.0              0.0              0.0               0.0  \n",
       "2                    0.0              0.0              0.0               0.0  \n",
       "3                    0.0              0.0              0.0               0.0  \n",
       "4                    0.0              0.0              0.0               0.0  \n",
       "..                   ...              ...              ...               ...  \n",
       "165                  0.0              0.0              0.0               0.0  \n",
       "166                  0.0              0.0              0.0               0.0  \n",
       "167                  0.0              0.0              0.0               0.0  \n",
       "168                  0.0              0.0              0.0               0.0  \n",
       "169                  0.0              0.0              0.0               0.0  \n",
       "\n",
       "[170 rows x 49 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolut love</th>\n",
       "      <th>also use</th>\n",
       "      <th>best price</th>\n",
       "      <th>brush teeth</th>\n",
       "      <th>ca wait</th>\n",
       "      <th>clean teeth</th>\n",
       "      <th>definit recommend</th>\n",
       "      <th>dri skin</th>\n",
       "      <th>even better</th>\n",
       "      <th>everi day</th>\n",
       "      <th>...</th>\n",
       "      <th>use skin</th>\n",
       "      <th>vox box</th>\n",
       "      <th>voxbox influenst</th>\n",
       "      <th>whole famili</th>\n",
       "      <th>whole grain</th>\n",
       "      <th>work great</th>\n",
       "      <th>work well</th>\n",
       "      <th>would definit</th>\n",
       "      <th>would recommend</th>\n",
       "      <th>year old</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1998 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      absolut love  also use  best price  brush teeth  ca wait  clean teeth  \\\n",
       "0              0.0       0.0         0.0          0.0      0.0          0.0   \n",
       "1              0.0       0.0         1.0          0.0      0.0          0.0   \n",
       "2              0.0       0.0         0.0          0.0      0.0          0.0   \n",
       "3              0.0       0.0         0.0          0.0      0.0          0.0   \n",
       "4              0.0       0.0         0.0          0.0      0.0          0.0   \n",
       "...            ...       ...         ...          ...      ...          ...   \n",
       "1993           0.0       0.0         0.0          0.0      0.0          0.0   \n",
       "1994           0.0       0.0         0.0          0.0      0.0          0.0   \n",
       "1995           0.0       0.0         0.0          0.0      0.0          0.0   \n",
       "1996           0.0       0.0         0.0          0.0      0.0          0.0   \n",
       "1997           0.0       0.0         0.0          0.0      0.0          0.0   \n",
       "\n",
       "      definit recommend  dri skin  even better  everi day  ...  use skin  \\\n",
       "0                   0.0       0.0          0.0        0.0  ...       0.0   \n",
       "1                   0.0       0.0          0.0        0.0  ...       0.0   \n",
       "2                   0.0       0.0          0.0        0.0  ...       0.0   \n",
       "3                   0.0       0.0          0.0        0.0  ...       0.0   \n",
       "4                   0.0       0.0          0.0        0.0  ...       0.0   \n",
       "...                 ...       ...          ...        ...  ...       ...   \n",
       "1993                0.0       0.0          0.0        0.0  ...       0.0   \n",
       "1994                0.0       0.0          0.0        0.0  ...       0.0   \n",
       "1995                0.0       0.0          0.0        0.0  ...       0.0   \n",
       "1996                0.0       0.0          0.0        0.0  ...       0.0   \n",
       "1997                0.0       0.0          0.0        0.0  ...       0.0   \n",
       "\n",
       "      vox box  voxbox influenst  whole famili  whole grain  work great  \\\n",
       "0         0.0               0.0           0.0          0.0         0.0   \n",
       "1         0.0               0.0           0.0          0.0         0.0   \n",
       "2         0.0               0.0           0.0          0.0         0.0   \n",
       "3         0.0               0.0           0.0          0.0         0.0   \n",
       "4         0.0               0.0           0.0          0.0         0.0   \n",
       "...       ...               ...           ...          ...         ...   \n",
       "1993      0.0               0.0           0.0          0.0         0.0   \n",
       "1994      0.0               0.0           0.0          0.0         0.0   \n",
       "1995      0.0               0.0           0.0          1.0         0.0   \n",
       "1996      0.0               0.0           0.0          0.0         0.0   \n",
       "1997      0.0               0.0           0.0          0.0         0.0   \n",
       "\n",
       "      work well  would definit  would recommend  year old  \n",
       "0           0.0            0.0              0.0       1.0  \n",
       "1           0.0            0.0              0.0       0.0  \n",
       "2           0.0            0.0              0.0       0.0  \n",
       "3           0.0            0.0              0.0       0.0  \n",
       "4           0.0            0.0              0.0       0.0  \n",
       "...         ...            ...              ...       ...  \n",
       "1993        0.0            0.0              0.0       0.0  \n",
       "1994        0.0            0.0              0.0       0.0  \n",
       "1995        0.0            0.0              0.0       0.0  \n",
       "1996        0.0            0.0              0.0       0.0  \n",
       "1997        0.0            0.0              0.0       0.0  \n",
       "\n",
       "[1998 rows x 89 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the TF-idf score for each tokens (n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_neg = neg_mx.T\n",
    "tf_neg['score'] = tf_neg.sum(axis =1)\n",
    "tf_pos = pos_mx.T\n",
    "tf_pos['score'] = tf_pos.sum(axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The features your analysis showed that customers cited as reasons for a good review\n",
    "##### Top 30 scores for positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>soft chewi</th>\n",
       "      <td>63.364194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love soft</th>\n",
       "      <td>53.399220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use hair</th>\n",
       "      <td>50.059379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year old</th>\n",
       "      <td>49.033671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highli recommend</th>\n",
       "      <td>42.803227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kid love</th>\n",
       "      <td>42.039136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mom voxbox</th>\n",
       "      <td>41.781764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tast like</th>\n",
       "      <td>39.038385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tast great</th>\n",
       "      <td>38.964118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolut love</th>\n",
       "      <td>38.952077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use cook</th>\n",
       "      <td>37.675886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realli like</th>\n",
       "      <td>33.817210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teeth clean</th>\n",
       "      <td>33.496552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also use</th>\n",
       "      <td>30.850444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clean teeth</th>\n",
       "      <td>29.539188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>individu wrap</th>\n",
       "      <td>28.408791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love love</th>\n",
       "      <td>28.084842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realli good</th>\n",
       "      <td>27.730302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use skin</th>\n",
       "      <td>27.621203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love treat</th>\n",
       "      <td>26.994730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great tast</th>\n",
       "      <td>26.943668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>receiv sampl</th>\n",
       "      <td>26.848966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great price</th>\n",
       "      <td>26.062708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tast good</th>\n",
       "      <td>25.774214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best price</th>\n",
       "      <td>25.274256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soft tast</th>\n",
       "      <td>25.230106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pet store</th>\n",
       "      <td>25.192852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get one</th>\n",
       "      <td>24.213809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>give one</th>\n",
       "      <td>24.002656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sampl influenst</th>\n",
       "      <td>23.888181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      score\n",
       "soft chewi        63.364194\n",
       "love soft         53.399220\n",
       "use hair          50.059379\n",
       "year old          49.033671\n",
       "highli recommend  42.803227\n",
       "kid love          42.039136\n",
       "mom voxbox        41.781764\n",
       "tast like         39.038385\n",
       "tast great        38.964118\n",
       "absolut love      38.952077\n",
       "use cook          37.675886\n",
       "realli like       33.817210\n",
       "teeth clean       33.496552\n",
       "also use          30.850444\n",
       "clean teeth       29.539188\n",
       "individu wrap     28.408791\n",
       "love love         28.084842\n",
       "realli good       27.730302\n",
       "use skin          27.621203\n",
       "love treat        26.994730\n",
       "great tast        26.943668\n",
       "receiv sampl      26.848966\n",
       "great price       26.062708\n",
       "tast good         25.774214\n",
       "best price        25.274256\n",
       "soft tast         25.230106\n",
       "pet store         25.192852\n",
       "get one           24.213809\n",
       "give one          24.002656\n",
       "sampl influenst   23.888181"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_pos_score = tf_pos[['score']].sort_values(by = 'score',ascending = False).head(30)\n",
    "tf_pos_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The features your analysis showed that customers cited as reasons for a poor review\n",
    "##### Top 30 scores for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>amazon vine program</th>\n",
       "      <td>2.240810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would take minut</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use skin hair</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>let start say</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good use kitchen</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get stuck throat</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open box found</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>box individu wrap</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mom use make</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calori gram fat</th>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>littl dri actual</th>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plastic contain label</th>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glass jar disappoint</th>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>six individu wrap</th>\n",
       "      <td>1.588320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whole grain oat</th>\n",
       "      <td>1.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>big glass milk</th>\n",
       "      <td>1.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would give star</th>\n",
       "      <td>1.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kid realli like</th>\n",
       "      <td>1.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realli like eat</th>\n",
       "      <td>1.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grain wheat flour</th>\n",
       "      <td>1.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>box amazon vine</th>\n",
       "      <td>1.315874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thrown away bewar</th>\n",
       "      <td>0.377964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poop becam green</th>\n",
       "      <td>0.377964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variou forum affect</th>\n",
       "      <td>0.377964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poop usual solid</th>\n",
       "      <td>0.377964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>read variou forum</th>\n",
       "      <td>0.377964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rice poop usual</th>\n",
       "      <td>0.377964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usual solid day</th>\n",
       "      <td>0.377964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop give thrown</th>\n",
       "      <td>0.377964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sure actual caus</th>\n",
       "      <td>0.377964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          score\n",
       "amazon vine program    2.240810\n",
       "would take minut       2.000000\n",
       "use skin hair          2.000000\n",
       "let start say          2.000000\n",
       "good use kitchen       2.000000\n",
       "get stuck throat       2.000000\n",
       "open box found         2.000000\n",
       "box individu wrap      2.000000\n",
       "mom use make           2.000000\n",
       "calori gram fat        1.707107\n",
       "littl dri actual       1.707107\n",
       "plastic contain label  1.707107\n",
       "glass jar disappoint   1.707107\n",
       "six individu wrap      1.588320\n",
       "whole grain oat        1.577350\n",
       "big glass milk         1.577350\n",
       "would give star        1.577350\n",
       "kid realli like        1.577350\n",
       "realli like eat        1.577350\n",
       "grain wheat flour      1.577350\n",
       "box amazon vine        1.315874\n",
       "thrown away bewar      0.377964\n",
       "poop becam green       0.377964\n",
       "variou forum affect    0.377964\n",
       "poop usual solid       0.377964\n",
       "read variou forum      0.377964\n",
       "rice poop usual        0.377964\n",
       "usual solid day        0.377964\n",
       "stop give thrown       0.377964\n",
       "sure actual caus       0.377964"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_neg_score = tf_neg[['score']].sort_values(by = 'score',ascending = False).head(30)\n",
    "tf_neg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The most common issues identified from your analysis that generated customer dissatisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In terms of product packaging, some pain points that emerged from this analysis were that open boxes of the product were delivered, the product was stuck together, and the glass jar design was disappointing to customers. Some ways the packaging issues can be handled are making sure the product is safe and secured in all steps of the delivery process. For the glass jars, it may be worth investigating this specific issue more to find if it’s worth investing money into a customer focus groups about the product design.\n",
    "- Some pain points about the products themselves that management can consider are that the products can get stuck in customer’s throats and that it turns poop green (which is likely an issue with the dog treats). To deal with these, management could consider changing the packaging to warn customers about these dangers or side-effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain to what degree the TF-IDF findings make sense - what are its limitations?\n",
    "\n",
    "- There are some words like ‘soft’ that can be represented as both positive and negative meanings. For example, for the cookie product, having a soft cookie might be better for the consumer, but for the dog food product, being too soft might present a negative review.\n",
    "- Using the TF-iDF improved the importance score of each term more than the count vectorizer, but when a review is scored as 3, more neutral side, the review can both have positive and negative words and can not weight those words or evaluate the sentiment for those words.\n",
    "- Since there are much less negative reviews than positive, a large n-gram size allows us to get more context, with the cost of the TF-IDF scores being lower. In general, if the n for the n-grams are small (2 or 3), it will limit the range of context we can obtain for the reviews, but will generate a higher score. On the other hand, if the n is large, it can capture more context to make it informative but the aggregated score will be lower which may not saw the importance when we look at the features analysis.\n",
    "- Another limitation with TF-IDF is that it doesn’t account for shifting sentiments within the same document. When manually investigating 3-star reviews, one thing we noticed was that many reviews would start with a positive sentiment (ie “the cookies are very good”), but then end with a negative sentiment (ie “but, they are too soft”), ultimately deeming the product as average and thus giving it a 3-star review. Since we grouped 3-star reviews under negative, any review with 3 stars is strictly seen as a negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity and Word Embeddings (2 pts)\n",
    "\n",
    "Using\n",
    "* `TfIdfVectorizer`\n",
    "\n",
    "Identify the most similar pair of reviews from the `amazon-fine-foods.csv` dataset using both Euclidean distance and cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cosine similarity function\n",
    "def similar_doc_cs(vec_df, ori_df):\n",
    "    \"\"\"\n",
    "    Inputs two arguments: vectorized dataframe and the original dataframe\n",
    "    Ouput is the two reviews that had the highest similarity and the score\n",
    "\n",
    "    This function uses the cosine similarity and retrieves the highest value besides 1\n",
    "    \"\"\"\n",
    "    cs_mx = cosine_similarity(vec_df, vec_df)\n",
    "    mod = np.where(cs_mx >= .99, 0, cs_mx)\n",
    "    indices = np.where(mod == mod.max())\n",
    "\n",
    "    count = 0\n",
    "    for n,i in enumerate(mod):\n",
    "        for m,j in enumerate(i):\n",
    "            if j >= mod[indices][0] - 0.001 and n < m and count < 1:\n",
    "                count += 1\n",
    "                print(f'The first review:', '\\n', ori_df.iloc[m,9], '\\n------------------------\\n', \n",
    "            f'The sencond review:', '\\n', ori_df.iloc[n,9])\n",
    "                print(f'The similarity score was {round(j,4)}')\n",
    "\n",
    "\n",
    "## euclidean distance function\n",
    "def similar_doc_ed(vec_df, ori_df):\n",
    "    \"\"\"\n",
    "    Inputs two arguments: vectorized dataframe and the original dataframe\n",
    "    Ouput is the two reviews that had the highest similarity and the score\n",
    "\n",
    "    This function uses the euclidean distance and retrieves the highest value besides 1\n",
    "    \"\"\"\n",
    "    cs_mx = euclidean_distances(vec_df, vec_df)\n",
    "    mod = np.where(cs_mx <= .009, 100, cs_mx)\n",
    "    indices = np.where(mod == mod.min())\n",
    "\n",
    "    count = 0\n",
    "    for n,i in enumerate(mod):\n",
    "        for m,j in enumerate(i):\n",
    "            if j <= mod[indices][0] and n < m and count < 1:\n",
    "                count += 1\n",
    "                print(f'The first review:', '\\n', ori_df.iloc[m,9], '\\n------------------------\\n', \n",
    "            f'The sencond review:', '\\n', ori_df.iloc[n,9])\n",
    "                print(f'The similarity score was {round(j,4)}')\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity for negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review: \n",
      " I received a box of these cookies through the Amazon Vine program, and I am not impressed.  I had rather high expectations since I generally like Quaker products, and my family eats a lot of their chewy granola bars. These cookies are quite disappointing. They are soft, but they are also very dry. The raisins are totally desiccated and not even chewy because they are so dry and shriveled. The cookies themselves have a dry, rather powdery texture. At 170 calories per cookie, I plan to stick with Quaker's chewy granola bars which taste much better and are in the 90-110 calorie range. \n",
      "------------------------\n",
      " The sencond review: \n",
      " I received one box of these cookies from the Amazon Vine program. I had not done any rsearch on them previously but remembered the oatmeal cookies my grandmother baked and how tasty they were. I was anxious to try out the Quaker version. When the box arrived the first thing I noticed, right on the front of the box, was that each of the six individually wrapped cookies contains a whopping 170 calories. When I was a super active kid I ate my grandmother's cookies with impunity although I did not know the calorie count. I was always a skinny kid regardless of what I ate. Now, as an adult, I am no longer skinny but not a lard butt either and I do not aspire to become one by eating these cookies. However, I wanted to do a taste test so I took a bite and quickly realized that these are simply run of the mill oatmeal cookies. They have no distinguishing characteristics other than the major calorie count. Dry, stale and tasteless are the best descriptions I can come up with.\n",
      "The similarity score was 0.8086\n"
     ]
    }
   ],
   "source": [
    "similar_doc_cs(neg_mx, neg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance for negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review: \n",
      " I received a box of these cookies through the Amazon Vine program, and I am not impressed.  I had rather high expectations since I generally like Quaker products, and my family eats a lot of their chewy granola bars. These cookies are quite disappointing. They are soft, but they are also very dry. The raisins are totally desiccated and not even chewy because they are so dry and shriveled. The cookies themselves have a dry, rather powdery texture. At 170 calories per cookie, I plan to stick with Quaker's chewy granola bars which taste much better and are in the 90-110 calorie range. \n",
      "------------------------\n",
      " The sencond review: \n",
      " I received one box of these cookies from the Amazon Vine program. I had not done any rsearch on them previously but remembered the oatmeal cookies my grandmother baked and how tasty they were. I was anxious to try out the Quaker version. When the box arrived the first thing I noticed, right on the front of the box, was that each of the six individually wrapped cookies contains a whopping 170 calories. When I was a super active kid I ate my grandmother's cookies with impunity although I did not know the calorie count. I was always a skinny kid regardless of what I ate. Now, as an adult, I am no longer skinny but not a lard butt either and I do not aspire to become one by eating these cookies. However, I wanted to do a taste test so I took a bite and quickly realized that these are simply run of the mill oatmeal cookies. They have no distinguishing characteristics other than the major calorie count. Dry, stale and tasteless are the best descriptions I can come up with.\n",
      "The similarity score was 0.6187\n"
     ]
    }
   ],
   "source": [
    "similar_doc_ed(neg_mx, neg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity for positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review: \n",
      " I bought a jar of this thing on Aug 16 for seven bucks (and free delivery if over Amazon's minimum for such). It came pretty fast, in a semi-liquid state but not leaking; rather well packaged. I let it melt completely in a warm room (CO melts at about 80 deg F) and then put it in the fridge to let it solidify. One of the reasons I did (and always do) that is to smooth out the texture: as is, every type of CO I've tried comes in like a bunch of white fibers stuck in more even liquid, but if you thaw it completely and then freeze, it becomes all evenly semi-transparent, like one thick candle sort of thing. It probably doesn't matter; just a personal quirk, I guess.<br /><br />Anyway, this is good stuff: the taste is nice -- very good for sweetich things like carrots or plums, for example (I always have a bowl of very lightly steamed carrots around, and I snack on them, taking one and dipping it into CO, which is good for two reasons: first, it tastes great, and second, carotene is fat-soluble so you need to combine carrots with some sort of fat or it won't be used by the body well).<br /><br />All in all, there isn't much to say -- CO is CO is CO; this kind is no different from other types I've tried. CO in general is shamelessly hyped as a sort of panacea from every health problem known to man; this is mostly lies, but two things are true: it is tasty, and it's good for skin/hair (I've tried both, and it's true, it works). Don't believe anything else and don't \"take\" it daily as if it were a medicine: it's pure saturated fat and that's the end of it: it will raise your cholesterol as any other kind of saturated fat. As far as its being a weight-loss agent, this is not a complete lie, but remember, it works only if you _substitute_ CO for other kinds of saturated fats (that means quit eating butter, meat, etc.) If, otoh, you simply _add_ it to your normal diet, you're not gonna lose any weight -- instead, you're gonna gain weight at least as good as you would otherwise + increase your cholesterol. But if you decide to substitute, do not think it's an easy choice: CO is pure saturated fat, whereas other sources of saturated fats (butter, meat, etc.) come with a lot of nutritional goodness like vitamins, proteins, and the like -- things that CO is wholly devoid of. So my suggestion, for what it's worth (not much, I know) is to use CO just as another agreeable food to be used in moderation, not a magic medicine. I read somewhere that a 40-sh male in good health on a 2000 cal/day diet can get 200 calories from saturated fats w/o harming himself.<br /><br />One other thing I wanted to mention: the jar says it's \"pure extra virgin\" -- keep in mind that \"extra virgin\" is meaningful only when applied to olive oil. That is because there are specific and stringent industry standards stipulating what constitutes \"virgin\", \"extra virgin\", etc. There are no such standards about any other oils, so saying your CO is \"extra virgin\" communicates zero useful information and is no more than disingenuous hand-waving. It's like Nutiva says on their jars \"no cholesterol!\". No kidding, it's like wood-free nails. Of course plant-derived foods have no cholesterol, but BS rules in this industry, so just be aware of what's what and smart enough to dismiss useless hype like claims of a CO being \"extra virgin\".<br /><br />OK, there's really no more to say: as long as you use this tasty substance rationally, it's good stuff, so bottomline is: recommended -- except for one thing: just as I was deciding it's time to post a review and navigating to this page, I noticed that the price went up by the mind-blowing 60 percent since, not just since I bought it (two weeks ago), but literally within the last few days. Does my salary go up 60 percent in a couple of days? No. So, while the product is good and I can't give it less than five stars, I will suggest you do not reward naked greed and at least try to find better-priced alternatives. Especially in this economic environment that we have right now. The current price here is even higher than it is in Wegman's (brick-and-mortar grocery store; they carry it, but it's more than the seven bucks I bought it for here).<br /><br />PS. Btw, I see now someone reports that this oil comes in a glass jar: this is a bare-faced lie. The jar is transparent plastic, quite soft actually, so you can't _honestly_ mistake it for glass. Be extremely suspicious about these massive five-star reviews (here and elsewhere); I for a long time have a feeling those are posted by manufacturers' employees to create hype (and why not? Amazon does nothing to guard against fake reviews; all one needs to do in order to start posting reviews (real OR fake) is buy something once from this site). Some reviews read like the reviewer hasn't actually seen the product in question and is interested only in adding another five-star rating, not the review.<br /><br />PPS. Thanks to commenters Lulu and Adriana for additional info and helpful suggestions. \n",
      "------------------------\n",
      " The sencond review: \n",
      " I have tried a few different extra virgin coconut oil products in the past (Vitamin shoppe brand, parachute,etc) and they tend to have a grainy texture to them. Not a major problem, but who wants to pick shredded coconut out of their hair? This brand was so silky, smooth, and smelled wonderful. I think I found my favorite brand of EVCO!\n",
      "The similarity score was 0.9863\n"
     ]
    }
   ],
   "source": [
    "similar_doc_cs(pos_mx, pos_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance for positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review: \n",
      " I bought a jar of this thing on Aug 16 for seven bucks (and free delivery if over Amazon's minimum for such). It came pretty fast, in a semi-liquid state but not leaking; rather well packaged. I let it melt completely in a warm room (CO melts at about 80 deg F) and then put it in the fridge to let it solidify. One of the reasons I did (and always do) that is to smooth out the texture: as is, every type of CO I've tried comes in like a bunch of white fibers stuck in more even liquid, but if you thaw it completely and then freeze, it becomes all evenly semi-transparent, like one thick candle sort of thing. It probably doesn't matter; just a personal quirk, I guess.<br /><br />Anyway, this is good stuff: the taste is nice -- very good for sweetich things like carrots or plums, for example (I always have a bowl of very lightly steamed carrots around, and I snack on them, taking one and dipping it into CO, which is good for two reasons: first, it tastes great, and second, carotene is fat-soluble so you need to combine carrots with some sort of fat or it won't be used by the body well).<br /><br />All in all, there isn't much to say -- CO is CO is CO; this kind is no different from other types I've tried. CO in general is shamelessly hyped as a sort of panacea from every health problem known to man; this is mostly lies, but two things are true: it is tasty, and it's good for skin/hair (I've tried both, and it's true, it works). Don't believe anything else and don't \"take\" it daily as if it were a medicine: it's pure saturated fat and that's the end of it: it will raise your cholesterol as any other kind of saturated fat. As far as its being a weight-loss agent, this is not a complete lie, but remember, it works only if you _substitute_ CO for other kinds of saturated fats (that means quit eating butter, meat, etc.) If, otoh, you simply _add_ it to your normal diet, you're not gonna lose any weight -- instead, you're gonna gain weight at least as good as you would otherwise + increase your cholesterol. But if you decide to substitute, do not think it's an easy choice: CO is pure saturated fat, whereas other sources of saturated fats (butter, meat, etc.) come with a lot of nutritional goodness like vitamins, proteins, and the like -- things that CO is wholly devoid of. So my suggestion, for what it's worth (not much, I know) is to use CO just as another agreeable food to be used in moderation, not a magic medicine. I read somewhere that a 40-sh male in good health on a 2000 cal/day diet can get 200 calories from saturated fats w/o harming himself.<br /><br />One other thing I wanted to mention: the jar says it's \"pure extra virgin\" -- keep in mind that \"extra virgin\" is meaningful only when applied to olive oil. That is because there are specific and stringent industry standards stipulating what constitutes \"virgin\", \"extra virgin\", etc. There are no such standards about any other oils, so saying your CO is \"extra virgin\" communicates zero useful information and is no more than disingenuous hand-waving. It's like Nutiva says on their jars \"no cholesterol!\". No kidding, it's like wood-free nails. Of course plant-derived foods have no cholesterol, but BS rules in this industry, so just be aware of what's what and smart enough to dismiss useless hype like claims of a CO being \"extra virgin\".<br /><br />OK, there's really no more to say: as long as you use this tasty substance rationally, it's good stuff, so bottomline is: recommended -- except for one thing: just as I was deciding it's time to post a review and navigating to this page, I noticed that the price went up by the mind-blowing 60 percent since, not just since I bought it (two weeks ago), but literally within the last few days. Does my salary go up 60 percent in a couple of days? No. So, while the product is good and I can't give it less than five stars, I will suggest you do not reward naked greed and at least try to find better-priced alternatives. Especially in this economic environment that we have right now. The current price here is even higher than it is in Wegman's (brick-and-mortar grocery store; they carry it, but it's more than the seven bucks I bought it for here).<br /><br />PS. Btw, I see now someone reports that this oil comes in a glass jar: this is a bare-faced lie. The jar is transparent plastic, quite soft actually, so you can't _honestly_ mistake it for glass. Be extremely suspicious about these massive five-star reviews (here and elsewhere); I for a long time have a feeling those are posted by manufacturers' employees to create hype (and why not? Amazon does nothing to guard against fake reviews; all one needs to do in order to start posting reviews (real OR fake) is buy something once from this site). Some reviews read like the reviewer hasn't actually seen the product in question and is interested only in adding another five-star rating, not the review.<br /><br />PPS. Thanks to commenters Lulu and Adriana for additional info and helpful suggestions. \n",
      "------------------------\n",
      " The sencond review: \n",
      " I have tried a few different extra virgin coconut oil products in the past (Vitamin shoppe brand, parachute,etc) and they tend to have a grainy texture to them. Not a major problem, but who wants to pick shredded coconut out of their hair? This brand was so silky, smooth, and smelled wonderful. I think I found my favorite brand of EVCO!\n",
      "The similarity score was 0.1657\n"
     ]
    }
   ],
   "source": [
    "similar_doc_ed(pos_mx, pos_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (3pts)\n",
    "\n",
    "You are an NLP data scientist working at Fandango. You observe the following dataset in your review comments:\n",
    "\n",
    "**Intent to Buy Tickets:**\n",
    "1.\tLove this movie. Can’t wait!\n",
    "2.\tI want to see this movie so bad.\n",
    "3.\tThis movie looks amazing.\n",
    "\n",
    "**No Intent to Buy Tickets:**\n",
    "1.\tLooks bad.\n",
    "2.\tHard pass to see this bad movie.\n",
    "3.\tSo boring!\n",
    "\n",
    "You can consider the following stopwords for removal: `to`, `this`.\n",
    "\n",
    "Is the following review an `Intent to Buy` or `No Intent to Buy`? Show your work for each computation.\n",
    "> This looks so bad.\n",
    "\n",
    "You'll need to compute:\n",
    "* Prior\n",
    "* Likelihood\n",
    "* Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prior:**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(y = Intent) &= 1/2   \\\\\n",
    "P(y = No Intent) &= 1/2 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "**Likelihood:**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(x \\mid y = Intent) & = P(x = \"looks\" \\mid y = Intent) * P(x = \"so\" \\mid y = Intent) * P(x = \"bad\" \\mid y = Intent)   \\\\\n",
    "& = (1/3) * (1/3) * (1/3) \\\\\n",
    "& = 1/3 * 1/3 * 1/3 \\\\\n",
    "& = 1/27 \\\\\n",
    "\\\\\n",
    "\\\\\n",
    "P(x \\mid y = No Intent) & = P(x = \"looks\" \\mid y = No Intent) * P(x = \"so\" \\mid y = No Intent) * P(x = \"bad\" \\mid y = No Intent)   \\\\\n",
    "& = (1/3) * (1/3) * (2/3) \\\\\n",
    "& = 1/3 * 1/3 * 2/3 \\\\\n",
    "& = 2/27 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "**Evidence:**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(x) &= P(x \\mid y = Intent)*P(y = Intent) + P(x \\mid y = No Intent)*P(y = No Intent)   \\\\\n",
    "& = (1/27) * (1/2) + (2/27) * (1/2) \\\\\n",
    "& = 1/18 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "**Posterior:**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(y = Intent \\mid x) &= (P(x \\mid y = Intent) * P(y = Intent))/ P(x)  \\\\\n",
    "& = ((1/27) * (1/2)) / (1/18) \\\\\n",
    "& = 1/3 \\\\\n",
    "\\\\\n",
    "\\\\\n",
    "P(y = No Intent \\mid x) &= (P(x \\mid y = No Intent) * P(y = No Intent))/ P(x) \\\\\n",
    "& = ((2/27) * (1/2)) / (1/18) \\\\\n",
    "& = 2/3 \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the posterior probability,  \"This looks so bad.\" will be classified as No Intent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
