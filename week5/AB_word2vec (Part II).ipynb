{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshaybhide/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/akshaybhide/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/akshaybhide/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/akshaybhide/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/akshaybhide/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/akshaybhide/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/akshaybhide/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/akshaybhide/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/akshaybhide/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/akshaybhide/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/akshaybhide/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/akshaybhide/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import en_core_web_sm\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>tag</th>\n",
       "      <th>dependency</th>\n",
       "      <th>shape</th>\n",
       "      <th>is_alphanumeric</th>\n",
       "      <th>is_stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Steve</td>\n",
       "      <td>Steve</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jobs</td>\n",
       "      <td>Jobs</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>Xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CC</td>\n",
       "      <td>cc</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Apple</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>conj</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>aux</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    text  lemma part_of_speech  tag dependency  shape  is_alphanumeric  \\\n",
       "0  Steve  Steve          PROPN  NNP   compound  Xxxxx             True   \n",
       "1   Jobs   Jobs          PROPN  NNP      nsubj   Xxxx             True   \n",
       "2    and    and          CCONJ   CC         cc    xxx             True   \n",
       "3  Apple  Apple          PROPN  NNP       conj  Xxxxx             True   \n",
       "4     is     be            AUX  VBZ        aux     xx             True   \n",
       "\n",
       "   is_stopword  \n",
       "0        False  \n",
       "1        False  \n",
       "2         True  \n",
       "3        False  \n",
       "4         True  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "rows = []\n",
    "doc = nlp(u\"Steve Jobs and Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    rows.append((token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop))\n",
    "    \n",
    "data = pd.DataFrame(rows, columns=[\"text\", \"lemma\", \"part_of_speech\", \"tag\", \"dependency\", \"shape\", \"is_alphanumeric\", \"is_stopword\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steve Jobs 0 10 PERSON\n",
      "Apple 15 20 ORG\n",
      "U.K. 42 46 GPE\n",
      "$1 billion 59 69 MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Steve Jobs and Apple is looking at buying U.K. startup for $1 billion\")\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve Jobs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking at buying \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.K.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " startup for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize this using displacy:\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings (word2vec Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Bag of Words (Use Context to Predict Target Word)\n",
    "![alt text](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/word2vec_cbow.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "![alt text](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/softmax.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram\n",
    "![alt text](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/skipgram.png \"Logo Title Text 1\")\n",
    "\n",
    "## Softmax\n",
    "![alt text](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/wordembedding_cluster.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dog - cat: 0.6064711809158325\n",
      " dog - Beijing: 0.26680809259414673\n",
      " dog - sad: 0.23861631751060486\n",
      " dog - depressed: 0.15567244589328766\n",
      " dog - couch: 0.40754207968711853\n",
      " dog - sofa: 0.28589534759521484\n",
      " dog - canine: 0.40234488248825073\n",
      " dog - China: 0.3280990421772003\n",
      " dog - Chinese: 0.223441019654274\n",
      " dog - France: 0.45519712567329407\n",
      " dog - Paris: 0.4896498918533325\n",
      " dog - banana: 0.37603437900543213\n",
      " cat - dog: 0.6064711809158325\n",
      " cat - Beijing: 0.34632179141044617\n",
      " cat - sad: 0.2476261705160141\n",
      " cat - depressed: 0.08602733165025711\n",
      " cat - couch: 0.2503412663936615\n",
      " cat - sofa: 0.25963470339775085\n",
      " cat - canine: 0.38918542861938477\n",
      " cat - China: 0.28076714277267456\n",
      " cat - Chinese: 0.11958666890859604\n",
      " cat - France: 0.3794781565666199\n",
      " cat - Paris: 0.39501628279685974\n",
      " cat - banana: 0.4623035490512848\n",
      " Beijing - dog: 0.26680809259414673\n",
      " Beijing - cat: 0.34632179141044617\n",
      " Beijing - sad: 0.2716704308986664\n",
      " Beijing - depressed: 0.22452880442142487\n",
      " Beijing - couch: 0.13369260728359222\n",
      " Beijing - sofa: 0.14226152002811432\n",
      " Beijing - canine: 0.0972571149468422\n",
      " Beijing - China: 0.5439580082893372\n",
      " Beijing - Chinese: 0.2218572497367859\n",
      " Beijing - France: 0.42475029826164246\n",
      " Beijing - Paris: 0.6329951286315918\n",
      " Beijing - banana: 0.19911344349384308\n",
      " sad - dog: 0.23861631751060486\n",
      " sad - cat: 0.2476261705160141\n",
      " sad - Beijing: 0.2716704308986664\n",
      " sad - depressed: 0.3655848801136017\n",
      " sad - couch: 0.07161746174097061\n",
      " sad - sofa: 0.06528615206480026\n",
      " sad - canine: 0.11390763521194458\n",
      " sad - China: 0.06946887820959091\n",
      " sad - Chinese: 0.2303980588912964\n",
      " sad - France: 0.16152942180633545\n",
      " sad - Paris: 0.14384041726589203\n",
      " sad - banana: 0.26627346873283386\n",
      " depressed - dog: 0.15567244589328766\n",
      " depressed - cat: 0.08602733165025711\n",
      " depressed - Beijing: 0.22452880442142487\n",
      " depressed - sad: 0.3655848801136017\n",
      " depressed - couch: 0.15045148134231567\n",
      " depressed - sofa: -0.03099963068962097\n",
      " depressed - canine: 0.19041414558887482\n",
      " depressed - China: 0.18923704326152802\n",
      " depressed - Chinese: 0.1768854856491089\n",
      " depressed - France: 0.1509069800376892\n",
      " depressed - Paris: 0.09740877896547318\n",
      " depressed - banana: -0.06587128341197968\n",
      " couch - dog: 0.40754207968711853\n",
      " couch - cat: 0.2503412663936615\n",
      " couch - Beijing: 0.13369260728359222\n",
      " couch - sad: 0.07161746174097061\n",
      " couch - depressed: 0.15045148134231567\n",
      " couch - sofa: 0.307026743888855\n",
      " couch - canine: 0.11578931659460068\n",
      " couch - China: 0.29890719056129456\n",
      " couch - Chinese: 0.3300574719905853\n",
      " couch - France: 0.29997438192367554\n",
      " couch - Paris: 0.15055692195892334\n",
      " couch - banana: 0.020790820941329002\n",
      " sofa - dog: 0.28589534759521484\n",
      " sofa - cat: 0.25963470339775085\n",
      " sofa - Beijing: 0.14226152002811432\n",
      " sofa - sad: 0.06528615206480026\n",
      " sofa - depressed: -0.03099963068962097\n",
      " sofa - couch: 0.307026743888855\n",
      " sofa - canine: 0.2369140386581421\n",
      " sofa - China: 0.05206387862563133\n",
      " sofa - Chinese: 0.1550963968038559\n",
      " sofa - France: 0.14373740553855896\n",
      " sofa - Paris: 0.370871901512146\n",
      " sofa - banana: 0.3729729950428009\n",
      " canine - dog: 0.40234488248825073\n",
      " canine - cat: 0.38918542861938477\n",
      " canine - Beijing: 0.0972571149468422\n",
      " canine - sad: 0.11390763521194458\n",
      " canine - depressed: 0.19041414558887482\n",
      " canine - couch: 0.11578931659460068\n",
      " canine - sofa: 0.2369140386581421\n",
      " canine - China: 0.03372814878821373\n",
      " canine - Chinese: -0.016037508845329285\n",
      " canine - France: 0.10409574210643768\n",
      " canine - Paris: 0.21894283592700958\n",
      " canine - banana: 0.22045570611953735\n",
      " China - dog: 0.3280990421772003\n",
      " China - cat: 0.28076714277267456\n",
      " China - Beijing: 0.5439580082893372\n",
      " China - sad: 0.06946887820959091\n",
      " China - depressed: 0.18923704326152802\n",
      " China - couch: 0.29890719056129456\n",
      " China - sofa: 0.05206387862563133\n",
      " China - canine: 0.03372814878821373\n",
      " China - Chinese: 0.36110448837280273\n",
      " China - France: 0.576897382736206\n",
      " China - Paris: 0.48589521646499634\n",
      " China - banana: 0.11915622651576996\n",
      " Chinese - dog: 0.223441019654274\n",
      " Chinese - cat: 0.11958666890859604\n",
      " Chinese - Beijing: 0.2218572497367859\n",
      " Chinese - sad: 0.2303980588912964\n",
      " Chinese - depressed: 0.1768854856491089\n",
      " Chinese - couch: 0.3300574719905853\n",
      " Chinese - sofa: 0.1550963968038559\n",
      " Chinese - canine: -0.016037508845329285\n",
      " Chinese - China: 0.36110448837280273\n",
      " Chinese - France: 0.4958896338939667\n",
      " Chinese - Paris: 0.32186079025268555\n",
      " Chinese - banana: -0.03121802769601345\n",
      " France - dog: 0.45519712567329407\n",
      " France - cat: 0.3794781565666199\n",
      " France - Beijing: 0.42475029826164246\n",
      " France - sad: 0.16152942180633545\n",
      " France - depressed: 0.1509069800376892\n",
      " France - couch: 0.29997438192367554\n",
      " France - sofa: 0.14373740553855896\n",
      " France - canine: 0.10409574210643768\n",
      " France - China: 0.576897382736206\n",
      " France - Chinese: 0.4958896338939667\n",
      " France - Paris: 0.5458804965019226\n",
      " France - banana: 0.118282750248909\n",
      " Paris - dog: 0.4896498918533325\n",
      " Paris - cat: 0.39501628279685974\n",
      " Paris - Beijing: 0.6329951286315918\n",
      " Paris - sad: 0.14384041726589203\n",
      " Paris - depressed: 0.09740877896547318\n",
      " Paris - couch: 0.15055692195892334\n",
      " Paris - sofa: 0.370871901512146\n",
      " Paris - canine: 0.21894283592700958\n",
      " Paris - China: 0.48589521646499634\n",
      " Paris - Chinese: 0.32186079025268555\n",
      " Paris - France: 0.5458804965019226\n",
      " Paris - banana: 0.36768609285354614\n",
      " banana - dog: 0.37603437900543213\n",
      " banana - cat: 0.4623035490512848\n",
      " banana - Beijing: 0.19911344349384308\n",
      " banana - sad: 0.26627346873283386\n",
      " banana - depressed: -0.06587128341197968\n",
      " banana - couch: 0.020790820941329002\n",
      " banana - sofa: 0.3729729950428009\n",
      " banana - canine: 0.22045570611953735\n",
      " banana - China: 0.11915622651576996\n",
      " banana - Chinese: -0.03121802769601345\n",
      " banana - France: 0.118282750248909\n",
      " banana - Paris: 0.36768609285354614\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(u'dog cat Beijing sad depressed couch sofa canine China Chinese France Paris banana')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        if token1 != token2:\n",
    "            print(f\" {token1} - {token2}: {1 - cosine(token1.vector, token2.vector)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Most Similar Words (Using Our Old Methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# inspect the default settings for CountVectorizer\n",
    "CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../week1/poor_amazon_toy_reviews.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ef44a595b30a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../week1/poor_amazon_toy_reviews.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m vectorizer = CountVectorizer(ngram_range=(1, 1), \n\u001b[1;32m      4\u001b[0m                              \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                              max_features=500,token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b')\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../week1/poor_amazon_toy_reviews.txt'"
     ]
    }
   ],
   "source": [
    "reviews = open(\"../week1/poor_amazon_toy_reviews.txt\").readlines()\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), \n",
    "                             stop_words=\"english\", \n",
    "                             max_features=500,token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b')\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "\n",
    "data = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# create similiarity matrix\n",
    "similarity_matrix = pd.DataFrame(cosine_similarity(data.T.values), \n",
    "             columns=vectorizer.get_feature_names(),\n",
    "                                 index=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'similarity_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3678d1c316ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# unstack matrix into table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msimilarity_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarity_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'similarity_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# unstack matrix into table\n",
    "similarity_table = similarity_matrix.rename_axis(None).rename_axis(None, axis=1).stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "similarity_table.columns = [\"word1\", \"word2\", \"similarity\"]\n",
    "similarity_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_table = similarity_table[similarity_table[\"similarity\"] < 0.99]\n",
    "similarity_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_table.sort_values(by=\"similarity\", ascending=False).drop_duplicates(\n",
    "    subset=\"similarity\", keep=\"first\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_500_words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Similar Words Using Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load into spacy your top 500 words\n",
    "\n",
    "tokens = nlp(f'{\" \".join(top_500_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "# create a list of similarity tuples\n",
    "\n",
    "similarity_tuples = []\n",
    "\n",
    "for token1, token2 in product(tokens, repeat=2):\n",
    "    similarity_tuples.append((token1, token2, token1.similarity(token2)))\n",
    "\n",
    "similarities = pd.DataFrame(similarity_tuples, columns=[\"word1\",\"word2\", \"score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find similar words\n",
    "similarities[similarities[\"score\"] < 1].sort_values(\n",
    "    by=\"score\", ascending=False).drop_duplicates(\n",
    "    subset=\"score\", keep=\"first\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Most Similar Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vectors for each review\n",
    "review_vectors = []\n",
    "NUM_REVIEWS = 400\n",
    "for review in reviews[:NUM_REVIEWS]:\n",
    "    sentence = nlp(review)\n",
    "    review_vectors.append(sentence.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df =pd.DataFrame(review_vectors)\n",
    "vector_df[\"text\"] = reviews[:NUM_REVIEWS]\n",
    "\n",
    "\n",
    "vector_df.set_index(\"text\", inplace=True)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarities = pd.DataFrame(cosine_similarity(vector_df.values), columns=reviews[:NUM_REVIEWS], index=reviews[:NUM_REVIEWS])\n",
    "\n",
    "top_similarities = similarities.unstack().reset_index()\n",
    "top_similarities.columns = [\"review1\", \"review2\", \"similarity\"]\n",
    "top_similarities = top_similarities.sort_values(by=\"similarity\", ascending=False)\n",
    "top_similarities = top_similarities[top_similarities[\"similarity\"] < .9999].head(10)\n",
    "\n",
    "\n",
    "for idx, row in top_similarities.iterrows():\n",
    "    print(row[\"review1\"])\n",
    "    print(row[\"review2\"])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshaybhide/opt/anaconda3/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/akshaybhide/opt/anaconda3/lib/python3.7/site-packages/seaborn/distributions.py:2103: FutureWarning: The `axis` variable is no longer used and will be removed. Instead, assign variables directly to `x` or `y`.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "vector = nlp(u'banana').vector\n",
    "\n",
    "ax = sns.distplot(vector, kde=False, rug=True)\n",
    "t = ax.set_title('Histogram of Feature Values')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Techniques\n",
    "\n",
    "### Subsampling\n",
    "\n",
    "What do we do with highly frequent words like `the` or `of`? We don't gain a ton of meaning from training on these words, and they become computationally expensive since they appear so frequently:\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/subsampling.png \"http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\")\n",
    "In the image above, $z(w_i)$ is the frequency of that particular word divided by the total number of words in the entire corpus. For instance, if a corpus of text has 50 words, and the word `dog` appears 3 times, $z(w_{dog}) = 0.06$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# write subsampling function\n",
    "def subsample(z):\n",
    "    return ((z * 1000) ** 0.5 + 1) * (0.001 / z)\n",
    "\n",
    "# plot this function:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Z = list(np.linspace(0,1,100))\n",
    "probability_of_keeping = list(map( lambda z: subsample(z), Z))\n",
    "\n",
    "plt.scatter(Z, probability_of_keeping)\n",
    "plt.xlabel(\"Frequency word appears in corpus\")\n",
    "plt.ylabel(\"Probability of keeping\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of Word Embeddings\n",
    "\n",
    "#### How to handle **Out Of Vocabulary (OOV)** words?\n",
    "Although **word2vec** and **FastText** include a significant vocabulary size, there will inevitably be words that are not included. For instance, if you are analyzing text conversations using word embeddings pretrained on Wikipedia text (which typically has more formal vocabulary than everyday language), how will you account for the following words?\n",
    "\n",
    "- DM\n",
    "- ROFLMAO\n",
    "- bae\n",
    "- ðŸ˜ƒ\n",
    "- #10YearChallenge\n",
    "- wut\n",
    "\n",
    "#### Potential solution: use word embeddings if they are available, and otherwise initialize the weights to random.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "def vectorize_word(input_word: str, D=50):\n",
    "    \"\"\"\n",
    "    D: an integer that represents the length (dimensionality of the word embeddings)\n",
    "    word_embeddings: A dictionary object with the string word as the key, and the embedding vector of \n",
    "    length D as the values.\n",
    "    For instance, word_embeddings[\"cat\"] will return [2.3, 4.5, 6.1, -2.2, ...]\n",
    "    \"\"\"\n",
    "    if input_word in word_embeddings.keys():\n",
    "        return word_embeddings[input_word]\n",
    "    else:\n",
    "        return np.random.rand(D)\n",
    "```\n",
    "\n",
    "##### Should we update the word embedding matrices during the model training step?\n",
    "- Ideally, you'd only want to be able to update the specific weights that were randomly initialized (since the rest of the weights are by definition pre-trained and are already pretty good). However, most deep learning libraries do not allow you to easily select which specific weight elements to apply backpropagation to- you either update all weights or you update none. In practice, most data scientists will \"freeze\" the word embedding layer:\n",
    "\n",
    "In Keras:\n",
    "```python\n",
    "word_embedding_layer.trainable = False # by default, trainable is set to true in Keras\n",
    "```\n",
    "In Tensorflow:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "N = 300 # number of words\n",
    "D = 50 # of dimensions in embeddings\n",
    "initial_word_embeddings = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "tensor = tf.constant(initial_word_embeddings, shape=[N, D])\n",
    "```\n",
    "\n",
    "- Ambiguity around **Domain-specific words**: using a generic pre-trained word embedding will not capture the semantic meaning of the word **sack** when it is used in the context of American football:\n",
    "![sack](images/football-bag-sack-diff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://radimrehurek.com/gensim/models/word2vec.html\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "path = get_tmpfile(\"word2vec.model\")\n",
    "model = Word2Vec(common_texts, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# define training data\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "             ['this', 'is', 'the', 'second', 'sentence'],\n",
    "             ['yet', 'another', 'sentence'],\n",
    "             ['one', 'more', 'sentence'],\n",
    "             ['and', 'the', 'final', 'sentence'],\n",
    "            [\"first\", \"and\", \"second\", \"sentence\"]]\n",
    "# train model\n",
    "# you can also specify an alpha, which is a hyperparameter learning rate\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary1\n",
    "model.wv.key_to_index\n",
    "model.wv.get_vector(\"sentence\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Your Own Word2Vec Embeddings Using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = open(\"../week1/good_amazon_toy_reviews.txt\").readlines() + open(\"../week1/poor_amazon_toy_reviews.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [word_tokenize(review) for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(docs, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"amazon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GoogleNews word2vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the entire Google News word embedding vectors\n",
    "from gensim.models import KeyedVectors\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "\n",
    "# word analogies\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most similar words for a target word\n",
    "model.most_similar(\"cappucino\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major problem with `word2vec` and other traditional word embedding strategies is how to deal with out of bag (OOB) or out of vocabulary (OOV) words.\n",
    "\n",
    "The vector embedding for the word `photosynthesis` will be the average of `pho`, `phot`, `photo`, etc.\n",
    "\n",
    "This allow FastText to embed many words that are not traditionally in the trained vocabulary (`photogenic` may be OOB, but `photo` will be available).\n",
    "\n",
    "### When to use?\n",
    "\n",
    "- traditionally, each individual word is trained onto a new word embedding\n",
    "- in many languages (including English), many words are morphologically derivative from each other. \n",
    "- use case when your corpus contains high-value, morphologically diverse, rare words (`photosynthesis`, `transcendentalism`)\n",
    "- may also be effective when your text contains lots of misspellings or abbreviations (ie. SMS, digital conversations)\n",
    "\n",
    "#### How is it Different Than word2vec?\n",
    "\n",
    "- word2vec considers only the entire word, whereas `fasttext` will consider each suffix n-gram.\n",
    "\n",
    "As [Radim Hurek](https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html) says:\n",
    "\n",
    "> The main principle behind fastText is that the morphological structure of a word carries important information about the meaning of the word. Such structure is not taken into account by traditional word embeddings like Word2Vec, which train a unique word embedding for every individual word. This is especially significant for morphologically rich languages (German, Turkish) in which a single word can have a large number of morphological forms, each of which might occur rarely, thus making it hard to train good word embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "model = fasttext.train_unsupervised(\n",
    "    '../week1/good_amazon_toy_reviews.txt', model='skipgram', lr=0.05, dim=100, ws=5, epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(model[\"adore\"].reshape(1,-1), model[\"love\"].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText Hyperparameters (From [Tutorial Notebook](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb))\n",
    "- **model**: Training architecture. Allowed values: `cbow`, `skipgram` (Default `cbow`)\n",
    "- **size**: Size of embeddings to be learnt (Default 100)\n",
    "- **alpha**: Initial learning rate (Default 0.025)\n",
    "- **window**: Context window size (Default 5)\n",
    "- **min_count**: Ignore words with number of occurrences below this (Default 5)\n",
    "- **loss**: Training objective. Allowed values: `ns`, `hs`, `softmax` (Default `ns`)\n",
    "- **sample**: Threshold for downsampling higher-frequency words (Default 0.001)\n",
    "- **negative**: Number of negative words to sample, for `ns` (Default 5)\n",
    "- **iter**: Number of epochs (Default 5)\n",
    "- **sorted_vocab**: Sort vocab by descending frequency (Default 1)\n",
    "- **threads**: Number of threads to use (Default 12)\n",
    "\n",
    "Hyperparameters unique to `fasttext`:\n",
    "- **min_n**: min length of char ngrams (Default 3)\n",
    "- **max_n**: max length of char ngrams (Default 6)\n",
    "- **bucket**: number of buckets used for hashing ngrams (Default 2000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Implementation of FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "text = list(pd.read_csv(\"bbc-text.csv\")[\"text\"].values)\n",
    "\n",
    "new_text = [word_tokenize(story) for story in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a New FastText Model Using the Corpus Available\n",
    "\n",
    "You can check the parameters available for you to tune [here](https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText.train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7053763, 9176110)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FastText(vector_size=40, window=3, min_count=1)  # change the size of the windows\n",
    "model.build_vocab(new_text)\n",
    "model.train(new_text, total_examples=len(new_text), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get corpus total count\n",
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.2222363e-01, -2.2078693e+00,  3.5745579e-01,  6.5984428e-01,\n",
       "       -2.0824301e+00, -1.5883364e-01,  7.4407870e-01, -2.9683098e-01,\n",
       "       -1.4233255e-01,  2.2058125e-01, -3.3079988e-01, -2.2909251e-01,\n",
       "        6.1040205e-01,  9.1151112e-01, -1.4807616e+00, -1.0192211e+00,\n",
       "        1.2085850e+00,  4.1515335e-01, -1.4587786e+00,  6.5934646e-01,\n",
       "       -2.9038453e-01, -1.4285205e-01, -1.5967873e-01,  4.7649127e-01,\n",
       "       -9.2041689e-01,  8.4571254e-01,  8.7594099e-02,  7.1397549e-01,\n",
       "        1.3489476e+00, -1.1265091e+00,  3.8475925e-01, -5.4505509e-01,\n",
       "       -1.4048903e-01,  1.2123585e-04,  7.0672899e-01,  1.4613642e+00,\n",
       "       -6.0756701e-01,  5.5079877e-01,  1.3566753e+00,  2.4346066e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get word vector for dog\n",
    "model.wv[\"dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get length of word embeddings\n",
    "len(model.wv[\"king\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('francesca', 0.9700016975402832), ('francesco', 0.9565298557281494), ('francique', 0.9212310910224915), ('franchisee', 0.9076738357543945), ('franck', 0.9072319269180298), ('melancholia', 0.9044543504714966), ('northumberland', 0.9014002680778503), ('icelandic', 0.9005484580993652), ('francs', 0.9004991054534912), ('holland', 0.898938000202179)]\n",
      "\n",
      "\n",
      "[('56-man', 0.9893273115158081), ('22-man', 0.9890055656433105), ('44-man', 0.9877821803092957), ('pitman', 0.9865984320640564), ('25-man', 0.9860122799873352), ('13-man', 0.9850900173187256), ('15-man', 0.9841371178627014), ('kidman', 0.9826959371566772), ('30-man', 0.9824084639549255), ('ryman', 0.9820998311042786)]\n",
      "\n",
      "\n",
      "[('transcript', 0.9709033966064453), ('transcripts', 0.9535253643989563), ('transmit', 0.9499515295028687), ('transform', 0.948307991027832), ('transsexual', 0.9453391432762146), ('translate', 0.9408653974533081), ('traffic', 0.9380313158035278), ('tracy', 0.9353316426277161), ('transforms', 0.9351500868797302), ('transit', 0.9341503977775574)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"france\"))\n",
    "print(\"\\n\")\n",
    "print(model.wv.most_similar(\"aquaman\"))\n",
    "print(\"\\n\")\n",
    "print(model.wv.most_similar(\"transc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Memory Version of Paragraph Vector (PV-DM)\n",
    "![](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/doc2vec.png)\n",
    "\n",
    "### Distributed Bag of Words of Paragraph Vector (PV-DBOW)\n",
    "![](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/doc2vec2.png)\n",
    "[A Gentle Introduction to Doc2Vec](https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(reviews)]\n",
    "model = Doc2Vec(documents, vector_size=50, window=4, min_count=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_vector = model.infer_vector([\"The\", \"toy\", \"was\", \"broken\", \"quickly\"]).reshape(1, -1)\n",
    "doc2_vector = model.infer_vector([\"It\", \"broke\", \"fast\"]).reshape(1, -1)\n",
    "doc3_vector = model.infer_vector([\"I ate lunch late\"]).reshape(1,-1)\n",
    "doc4_vector = model.infer_vector([\"It\", \"was\", \"crappy\", \"quality\"]).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01620534]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(doc1_vector, doc2_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4734929]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(doc1_vector, doc3_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23254903]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(doc2_vector, doc3_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11511255]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(doc3_vector, doc4_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
